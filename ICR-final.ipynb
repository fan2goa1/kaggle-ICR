{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ICR Competition","metadata":{"execution":{"iopub.execute_input":"2023-06-25T10:08:23.661832Z","iopub.status.busy":"2023-06-25T10:08:23.66068Z","iopub.status.idle":"2023-06-25T10:08:44.412009Z","shell.execute_reply":"2023-06-25T10:08:44.409631Z","shell.execute_reply.started":"2023-06-25T10:08:23.661787Z"},"papermill":{"duration":0.012759,"end_time":"2023-07-14T18:25:17.471643","exception":false,"start_time":"2023-07-14T18:25:17.458884","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# run_para = 'local'      # local用于本地跑；kaggle用于在kaggle上评测\nrun_para = 'kaggle'\n\nFeature_Derivation = {\n    \"Non_feature\": 0,  # 缺失值特征衍生\n    \"Demaxmin\": 0,  # 最大最小值特征衍生\n    \"K_dist\": 0,  # 到聚类中心的距离特征衍生\n    \"Maxmin_dist\": 1,  # 到聚类中心dist max (min) mean特征衍生\n    \"BN_bin\": 1,  # BN 分箱特征衍生\n    \"Poly\": 1,  # 多项式特征衍生\n}\nPost_Process = {\n    \"Calibration\": 'none',  # 校准类型(bc: Beat; iso: Isotonic)\n    \"Set_thres\": False,  # 是否使用卡阈值\n    \"boost\": 1.0,         # boost\n    \"optimal\": False,      # from the discussion\n    \"trival_calib\": True,  # 0.5 0.5 calibration\n    \"Ensemble_method\": 'mean', # how to ensemble the models' predicted probabilities together.\n}\n\nthres_high_l = 1000.0\nthres_high_r = 1000.0\nthres_low_l = 0.05\nthres_low_r = 0.20","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:08.871084Z","iopub.execute_input":"2023-08-10T13:28:08.871686Z","iopub.status.idle":"2023-08-10T13:28:08.879707Z","shell.execute_reply.started":"2023-08-10T13:28:08.871650Z","shell.execute_reply":"2023-08-10T13:28:08.878548Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"markdown","source":"## Enviroment Setup\n\n先配置环境，在本地时候不需要执行下面这个代码块，在kaggle中需要。","metadata":{}},{"cell_type":"code","source":"if run_para == 'kaggle':\n    !pip install tabpfn --no-index --find-links=file:///kaggle/input/pip-packages-icr/pip-packages\n    !mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n    !cp /kaggle/input/pip-packages-icr/pip-packages/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/\n#     !pip install betacal --no-index --find-links=file:///kaggle/input/betacal-whl","metadata":{"papermill":{"duration":18.318056,"end_time":"2023-07-14T18:25:35.799931","exception":false,"start_time":"2023-07-14T18:25:17.481875","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-10T13:28:08.882677Z","iopub.execute_input":"2023-08-10T13:28:08.883805Z","iopub.status.idle":"2023-08-10T13:28:23.099241Z","shell.execute_reply.started":"2023-08-10T13:28:08.883768Z","shell.execute_reply":"2023-08-10T13:28:23.097809Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stdout","text":"Looking in links: file:///kaggle/input/pip-packages-icr/pip-packages\nRequirement already satisfied: tabpfn in /opt/conda/lib/python3.10/site-packages (0.1.9)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (1.23.5)\nRequirement already satisfied: pyyaml>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (6.0)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (2.31.0)\nRequirement already satisfied: scikit-learn>=0.24.2 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (1.2.2)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from tabpfn) (2.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (2023.5.7)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (1.11.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->tabpfn) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->tabpfn) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder,normalize\nfrom sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import LeaveOneOut\nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nimport xgboost\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport inspect\nfrom collections import defaultdict\nfrom tabpfn import TabPFNClassifier\nimport warnings\nimport torch\n\nwarnings.filterwarnings(\"ignore\")\n# warnings.filterwarnings(action='ignore', category=LightGBMWarning)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":5.82459,"end_time":"2023-07-14T18:25:41.635568","exception":false,"start_time":"2023-07-14T18:25:35.810978","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-10T13:28:23.102022Z","iopub.execute_input":"2023-08-10T13:28:23.102443Z","iopub.status.idle":"2023-08-10T13:28:23.115448Z","shell.execute_reply.started":"2023-08-10T13:28:23.102404Z","shell.execute_reply":"2023-08-10T13:28:23.114413Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"markdown","source":"## Read Data & EDA\n### 简单处理\n先加载各个数据文件","metadata":{"papermill":{"duration":0.010749,"end_time":"2023-07-14T18:25:41.657879","exception":false,"start_time":"2023-07-14T18:25:41.647130","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nif run_para == 'kaggle':\n    train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n    test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n    sample = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n    greeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\nelif run_para == 'local':\n    train = pd.read_csv('./icr-identify-age-related-conditions/train.csv')\n    test = pd.read_csv('./icr-identify-age-related-conditions/test.csv')\n    sample = pd.read_csv('./icr-identify-age-related-conditions/sample_submission.csv')\n    greeks = pd.read_csv('./icr-identify-age-related-conditions/greeks.csv')\n\n# hard_samples = [102, 267, 313, 318, 434, 509]\n\n# train = train.drop(hard_samples)\n# greeks = greeks.drop(hard_samples)\n\n# train.reset_index(drop=True, inplace=True)\n# greeks.reset_index(drop=True, inplace=True)    \n\nprint(train.shape)","metadata":{"papermill":{"duration":0.088192,"end_time":"2023-07-14T18:25:41.756866","exception":false,"start_time":"2023-07-14T18:25:41.668674","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-10T13:28:23.118967Z","iopub.execute_input":"2023-08-10T13:28:23.119337Z","iopub.status.idle":"2023-08-10T13:28:23.489036Z","shell.execute_reply.started":"2023-08-10T13:28:23.119307Z","shell.execute_reply":"2023-08-10T13:28:23.488003Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stdout","text":"(617, 58)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"然后将EJ属性二值化（0/1）。","metadata":{"papermill":{"duration":0.010591,"end_time":"2023-07-14T18:25:41.780367","exception":false,"start_time":"2023-07-14T18:25:41.769776","status":"completed"},"tags":[]}},{"cell_type":"code","source":"first_category = train.EJ.unique()[0]\ntrain.EJ = train.EJ.eq(first_category).astype('int')\ntest.EJ = test.EJ.eq(first_category).astype('int')","metadata":{"papermill":{"duration":0.029546,"end_time":"2023-07-14T18:25:41.820957","exception":false,"start_time":"2023-07-14T18:25:41.791411","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-10T13:28:23.490369Z","iopub.execute_input":"2023-08-10T13:28:23.490752Z","iopub.status.idle":"2023-08-10T13:28:23.502524Z","shell.execute_reply.started":"2023-08-10T13:28:23.490717Z","shell.execute_reply":"2023-08-10T13:28:23.501457Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"markdown","source":"### 数据整数化\n根据Disscussion Post，我们可以将原数据集中的某些属性转化成整数取值，并且猜测BN属性可能是指年龄，所以我们可以在BN上进行一些操作。","metadata":{"papermill":{"duration":0.010485,"end_time":"2023-07-14T18:25:41.842451","exception":false,"start_time":"2023-07-14T18:25:41.831966","status":"completed"},"tags":[]}},{"cell_type":"code","source":"int_denominators = {\n    'AB': 0.004273,\n    'AF': 0.00242,\n    'AH': 0.008709,\n    'AM': 0.003097,\n    'AR': 0.005244,\n    'AX': 0.008859,\n    'AY': 0.000609,\n    'AZ': 0.006302,\n    'BC': 0.007028,\n    'BD ': 0.00799,\n    'BN': 0.3531,\n    'BP': 0.004239,\n    'BQ': 0.002605,\n    'BR': 0.006049,\n    'BZ': 0.004267,\n    'CB': 0.009191,\n    'CC': 6.12e-06,\n    'CD ': 0.007928,\n    'CF': 0.003041,\n    'CH': 0.000398,\n    'CL': 0.006365,\n    'CR': 7.5e-05,\n    'CS': 0.003487,\n    'CU': 0.005517,\n    'CW ': 9.2e-05,\n    'DA': 0.00388,\n    'DE': 0.004435,\n    'DF': 0.000351,\n    'DH': 0.002733,\n    'DI': 0.003765,\n    'DL': 0.00212,\n    'DN': 0.003412,\n    'DU': 0.0013794,\n    'DV': 0.00259,\n    'DY': 0.004492,\n    'EB': 0.007068,\n    'EE': 0.004031,\n    'EG': 0.006025,\n    'EH': 0.006084,\n    'EL': 0.000429,\n    'EP': 0.009269,\n    'EU': 0.005064,\n    'FC': 0.005712,\n    'FD ': 0.005937,\n    'FE': 0.007486,\n    'FI': 0.005513,\n    'FR': 0.00058,\n    'FS': 0.006773,\n    'GB': 0.009302,\n    'GE': 0.004417,\n    'GF': 0.004374,\n    'GH': 0.003721,\n    'GI': 0.002572\n}\nfor k, v in int_denominators.items():\n    train[k] = np.round(train[k] / v, 1)\n    test[k] = np.round(test[k] / v, 1)","metadata":{"papermill":{"duration":0.078179,"end_time":"2023-07-14T18:25:41.931465","exception":false,"start_time":"2023-07-14T18:25:41.853286","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-10T13:28:23.504588Z","iopub.execute_input":"2023-08-10T13:28:23.505162Z","iopub.status.idle":"2023-08-10T13:28:23.573237Z","shell.execute_reply.started":"2023-08-10T13:28:23.505122Z","shell.execute_reply":"2023-08-10T13:28:23.572218Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"markdown","source":"对一些属性特征进行drop操作，尝试找到最佳的组合方式。","metadata":{}},{"cell_type":"code","source":"ft = ['AF','BQ','AB','DU','DI','FL','CR','DH','DA','EH','CD ','BP','BC','DL','EE','FD ','DE','GL','FR','FI','EB','CU','CS', 'BN']\n# train = train[['Id'] + ft + ['Class']]\n# test = test[['Id'] + ft]\n\npure_train = train.drop(['Id', 'Class'], axis=1)\npure_test = test.drop(['Id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:23.574870Z","iopub.execute_input":"2023-08-10T13:28:23.575203Z","iopub.status.idle":"2023-08-10T13:28:23.588208Z","shell.execute_reply.started":"2023-08-10T13:28:23.575170Z","shell.execute_reply":"2023-08-10T13:28:23.587204Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"markdown","source":"### 记录每条数据缺失值个数\n\n在数据集中填充记录每条数据有多少缺失值。","metadata":{}},{"cell_type":"code","source":"'''\nnon_feature: 表示该位置的值是否为缺失值\nnon_feature_count: 表示该行（一条数据）缺失值的个数\n'''\ndef creat_non_feature(df, columns):\n    feature_isnull_col = [f'{f}_isnull' for f in columns]\n    non_feature = df[columns].isnull().astype(int)\n    non_feature.columns = feature_isnull_col\n    \n    non_feature_count = pd.DataFrame(non_feature.sum(axis=1))\n    non_feature_count.columns = ['count_isnull']\n    \n    non_feature = non_feature.reset_index(drop=True)\n    non_feature_count = non_feature_count.reset_index(drop=True)\n    return non_feature, non_feature_count\n\ncolumns = pure_train.columns.tolist()\nnon_feature, non_feature_count = creat_non_feature(pure_train, columns)\nif Feature_Derivation['Non_feature']:\n    train = pd.concat([train, non_feature_count], axis=1)\n\nnon_feature, non_feature_count = creat_non_feature(pure_test, columns)\nif Feature_Derivation['Non_feature']:\n    test = pd.concat([test, non_feature_count], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:23.590130Z","iopub.execute_input":"2023-08-10T13:28:23.590518Z","iopub.status.idle":"2023-08-10T13:28:23.608206Z","shell.execute_reply.started":"2023-08-10T13:28:23.590469Z","shell.execute_reply":"2023-08-10T13:28:23.607184Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"markdown","source":"### 填补缺失值\n\n在这里我们使用每个特征数据的中位数填补缺失值。","metadata":{}},{"cell_type":"code","source":"Imp = SimpleImputer(missing_values=np.nan, strategy='median')\n\ncolumns_to_select = [col for col in train.columns if col not in ['Class', 'Id']]\n\ntrain_data = train[columns_to_select].copy()\ntest_data = test[columns_to_select].copy()\n\n# 填充缺失值\ntrain_data = pd.DataFrame(Imp.fit_transform(train_data), columns=columns_to_select)\ntest_data = pd.DataFrame(Imp.transform(test_data), columns=columns_to_select)\n\n# 重新组合数据和原始列\ntrain_filled = pd.concat([train['Id'], train_data, train['Class']], axis=1)\ntest_filled = pd.concat([test['Id'], test_data], axis=1)\n\ntrain = train_filled.copy()\ntest = test_filled.copy()\n\npure_train = train[columns]\npure_test = test[columns]\n# print(train.shape, test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:23.609914Z","iopub.execute_input":"2023-08-10T13:28:23.610328Z","iopub.status.idle":"2023-08-10T13:28:23.638680Z","shell.execute_reply.started":"2023-08-10T13:28:23.610295Z","shell.execute_reply":"2023-08-10T13:28:23.637717Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"markdown","source":"### 聚类距离distance\n我们对Class为0和Class为1的数据分别进行KMeans聚类，然后计算每个数据点到各个中心的距离。","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\ndef feature_clustering(df, label, k, ref_label):\n    kmeans = KMeans(n_clusters=k, init='random', n_init=30, max_iter=200, tol=0.001)\n    kmeans.fit(df[label == ref_label])\n    return kmeans.cluster_centers_\n\n# 计算距离各聚类中心点的欧式距离\ndef dist_transform(df, centers, ref_label):\n    final_data=pd.DataFrame()    \n    for i in range(len(centers)):\n        final_data[f\"feature_distance_{ref_label}_{i}\"] = np.linalg.norm(df - centers[i], axis=1)\n    return final_data\n\ntrain_label = train['Class']\nif Feature_Derivation['K_dist']:\n    # train数据集的聚类计算\n    centers_0 = feature_clustering(pure_train, train_label, 20, 0)\n    dist_feature_0 = dist_transform(pure_train, centers_0, 0)\n    centers_1 = feature_clustering(pure_train, train_label, 15, 1)\n    dist_feature_1 = dist_transform(pure_train, centers_1, 1)\n    train = pd.concat([train, dist_feature_0, dist_feature_1], axis=1)\n\n    # test数据集的聚类计算\n    dist_feature_0 = dist_transform(pure_test, centers_0, 0)\n    dist_feature_1 = dist_transform(pure_test, centers_1, 1)\n    test = pd.concat([test, dist_feature_0, dist_feature_1], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:23.643414Z","iopub.execute_input":"2023-08-10T13:28:23.643739Z","iopub.status.idle":"2023-08-10T13:28:23.658633Z","shell.execute_reply.started":"2023-08-10T13:28:23.643711Z","shell.execute_reply":"2023-08-10T13:28:23.657591Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"markdown","source":"除了计算到每个聚类中心的距离，我们可以提取出每个数据点到Class0/1的聚类中心的距离的最大、最小、平均值。","metadata":{}},{"cell_type":"code","source":"def get_dis_feature(df, centers, i):\n    dist_feature = dist_transform(df, centers, i)  # 所点距离正常类样本中心的距离（32个中心）\n    min_dist_feature = dist_feature.min(axis=1)\n    min_dist_feature.name = f\"min_dist_feature_{i}\"\n\n    max_dist_feature = dist_feature.max(axis=1)\n    max_dist_feature.name  = f\"max_dist_feature_{i}\"\n\n    mean_dist_feature = dist_feature.mean(axis=1)\n    mean_dist_feature.name  = f\"mean_dist_feature_{i}\"\n    return min_dist_feature, max_dist_feature, mean_dist_feature\n\nif Feature_Derivation['Maxmin_dist']:\n    # train数据集\n    train_label = train['Class']\n    centers_0 = feature_clustering(pure_train, train_label, 32, 0)\n    train_min_dist_0, train_max_dist_0, train_mean_dist_0 = get_dis_feature(pure_train, centers_0, 0)\n    centers_1 = feature_clustering(pure_train, train_label, 32, 1)\n    train_min_dist_1, train_max_dist_1, train_mean_dist_1 = get_dis_feature(pure_train, centers_1, 1)\n    # train = pd.concat([train, train_min_dist_0, train_max_dist_0, train_mean_dist_0, train_min_dist_1, train_max_dist_1, train_mean_dist_1], axis=1)\n    train = pd. concat([train, train_min_dist_0, train_min_dist_1], axis=1)\n\n    # test数据集\n    test_min_dist_0, test_max_dist_0, test_mean_dist_0 = get_dis_feature(pure_test, centers_0, 0)\n    test_min_dist_1, test_max_dist_1, test_mean_dist_1 = get_dis_feature(pure_test, centers_1, 1)\n    # test = pd.concat([test, test_min_dist_0, test_max_dist_0, test_mean_dist_0, test_min_dist_1, test_max_dist_1, test_mean_dist_1], axis=1)\n    test = pd.concat([test, test_min_dist_0, test_min_dist_1], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:23.660007Z","iopub.execute_input":"2023-08-10T13:28:23.660369Z","iopub.status.idle":"2023-08-10T13:28:24.090970Z","shell.execute_reply.started":"2023-08-10T13:28:23.660329Z","shell.execute_reply":"2023-08-10T13:28:24.089899Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"markdown","source":"### 分箱（BN列）\n通过绘制密度-年龄图我们可以看到，Class为1的样本对应的曲线比Class为0的样本对应的曲线要右移了一些，所以这体现出年龄较大时更可能得病。故我们可以在BN上进行一些分析和处理：对BN列进行聚类分箱，总共分为5-6类，增加新列BN_binning。","metadata":{"papermill":{"duration":0.01046,"end_time":"2023-07-14T18:25:41.952621","exception":false,"start_time":"2023-07-14T18:25:41.942161","status":"completed"},"tags":[]}},{"cell_type":"code","source":"k = 7\nif run_para == 'kaggle':\n    BNpd = pd.concat([train['BN'], test['BN']], axis=0, ignore_index=True)\nelif run_para == 'local':\n    BNpd = train['BN']\n\nBNpd = pd.concat([train['BN'], test['BN']], axis=0, ignore_index=True)\ndata = BNpd.values.reshape(-1, 1)\nkmodel = KMeans(n_clusters=k)           # k为聚成几类\nkmodel.fit(data)  # 训练模型\nc = pd.DataFrame(kmodel.cluster_centers_, columns=['cc']) # 求聚类中心\nc0 = pd.DataFrame({'cc': [0.0]})\nc = pd.concat([c0, c], axis=0, ignore_index=True)\nc = c.sort_values(by='cc').reset_index(drop=True)\n\n# 求聚类中心之间的平均值作为分割点\nfor i in range(c.shape[0] - 1):\n    c.iloc[i]['cc'] = (c.iloc[i]['cc'] + c.iloc[i+1]['cc']) / 2\nc = c.drop(c.index[-1])\n\nc0 = pd.DataFrame({'cc': [0.0]})\ncn = pd.DataFrame({'cc': [max(train['BN'].max(), test['BN'].max()) * 5]})\nc = pd.concat([c0, c, cn], axis=0, ignore_index=True)\nc = c['cc'].round().astype(int)\nc = c.unique()\nrange_num = c.shape[0] - 1\nc = c.tolist()\n\n# 保留旧BN，添加BN_binning\ntrain_BN = train['BN'].values\ntrain_binning = pd.cut(train_BN, c, labels=range(range_num), include_lowest=True).astype(int)\nif Feature_Derivation['BN_bin']:\n    train['BN_binning'] = train_binning\n\ntest_BN = test['BN'].values\ntest_binning = pd.cut(test_BN, c, labels=range(range_num), include_lowest=True).astype(int)\nif Feature_Derivation['BN_bin']:\n    test['BN_binning'] = test_binning","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.092480Z","iopub.execute_input":"2023-08-10T13:28:24.092871Z","iopub.status.idle":"2023-08-10T13:28:24.146020Z","shell.execute_reply.started":"2023-08-10T13:28:24.092825Z","shell.execute_reply":"2023-08-10T13:28:24.145191Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"markdown","source":"### 其他特征衍生\n\n#### 多项式特征衍生\n对DI、DU和Br、Bz进行二阶多项式特征衍生，","metadata":{}},{"cell_type":"code","source":"Multiply_features = [\n#     ['DI', 'DU'],\n#     ['DU', 'DU'],\n    ['DU', 'FR'],\n    ['DA', 'DE'],\n    ['AB', 'GL'],\n]\n\nfor j, columns_to_mul in enumerate(Multiply_features):\n    mix_col = columns_to_mul[0] + '+' + columns_to_mul[1]\n    train[mix_col] = train[columns_to_mul[0]] * train[columns_to_mul[1]]\n    test[mix_col] = test[columns_to_mul[0]] * test[columns_to_mul[1]]","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.149945Z","iopub.execute_input":"2023-08-10T13:28:24.152019Z","iopub.status.idle":"2023-08-10T13:28:24.163965Z","shell.execute_reply.started":"2023-08-10T13:28:24.151986Z","shell.execute_reply":"2023-08-10T13:28:24.162816Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"def change(X):\n    X['out_GL'] = 0\n    X.loc[X['GL']<1,'out_GL'] = X.loc[X['GL']<1,'GL'].map(lambda x : x-X.loc[X['GL']<1,'GL'].mean())\n    X.loc[X['GL']>1.5,'out_GL'] = X.loc[X['GL']>1.5,'GL'].map(lambda x : x-X.loc[X['GL']>1.5,'GL'].mean())\n    X.out_GL = X.out_GL.astype('float')\n    X['DA*CS'] = np.log(X.DA*2 / X.CS**0.5)#0.2100892\\\n    return X","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.167451Z","iopub.execute_input":"2023-08-10T13:28:24.167833Z","iopub.status.idle":"2023-08-10T13:28:24.176288Z","shell.execute_reply.started":"2023-08-10T13:28:24.167803Z","shell.execute_reply":"2023-08-10T13:28:24.175040Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"train = change(train)\ntest = change(test)\nprint(train.columns)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.178181Z","iopub.execute_input":"2023-08-10T13:28:24.178564Z","iopub.status.idle":"2023-08-10T13:28:24.411859Z","shell.execute_reply.started":"2023-08-10T13:28:24.178528Z","shell.execute_reply":"2023-08-10T13:28:24.410854Z"},"trusted":true},"execution_count":159,"outputs":[{"name":"stdout","text":"Index(['Id', 'AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n       'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n       'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n       'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n       'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL', 'Class',\n       'min_dist_feature_0', 'min_dist_feature_1', 'BN_binning', 'DU+FR',\n       'DA+DE', 'AB+GL', 'out_GL', 'DA*CS'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\n\nPoly_features = [\n                 ['DI', 'DU'],\n#                  ['BR', 'BZ'],\n#                  ['CR', 'AB', 'FL',],\n#                  ['CR'],\n                ]\n\nfor j, columns_to_derive in enumerate(Poly_features):\n    # 多项式特征衍生\n    degree_dim = 2\n    poly = PolynomialFeatures(degree=degree_dim, include_bias=False, interaction_only=False)\n    # Z-Score标准化\n    scaler = StandardScaler()\n    \n    poly_features = poly.fit_transform(train[columns_to_derive])\n    scaled_features = scaler.fit_transform(poly_features)\n    # 生成新的特征列名\n    new_feature_names = [f\"poly_{j}_{i}\" for i in range(scaled_features.shape[1])]\n    features_train_df = pd.DataFrame(scaled_features, columns=new_feature_names)\n    if Feature_Derivation['Poly']:\n        train = pd.concat([train, features_train_df], axis=1)\n        train = train.drop(columns=columns_to_derive)\n        for x in columns_to_derive:\n            ft.remove(x)\n\n    # 测试集\n    poly_features_test = poly.transform(test[columns_to_derive])\n    scaled_features_test = scaler.transform(poly_features_test)\n    features_test_df = pd.DataFrame(scaled_features_test, columns=new_feature_names)\n    if Feature_Derivation['Poly']:\n        test = pd.concat([test, features_test_df], axis=1)\n        test = test.drop(columns=columns_to_derive)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.414548Z","iopub.execute_input":"2023-08-10T13:28:24.415272Z","iopub.status.idle":"2023-08-10T13:28:24.437470Z","shell.execute_reply.started":"2023-08-10T13:28:24.415234Z","shell.execute_reply":"2023-08-10T13:28:24.436315Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"drop_col = [\n    ['CL'],\n    # ['EU', 'CW '],\n    # ['count_isnull'],\n    # ['feature_distance_0_0', 'feature_distance_0_1', 'feature_distance_0_2', 'feature_distance_0_3', 'feature_distance_0_4', 'feature_distance_0_5', 'feature_distance_0_6', 'feature_distance_0_7', 'feature_distance_0_8', 'feature_distance_0_9'],\n    # ['feature_distance_1_0', 'feature_distance_1_1', 'feature_distance_1_2', 'feature_distance_1_3', 'feature_distance_1_4', 'feature_distance_1_5', 'feature_distance_1_6', 'feature_distance_1_7', 'feature_distance_1_8', 'feature_distance_1_9'],\n    # ['BN_binning'],\n#     ['CF', 'AF', 'FE', 'CR', 'BR', 'GH', 'EE']\n    ['FD ','CS'],\n    ['CW ', 'DV'],\n    ['BD ', 'AR'],\n]\nfor dc in drop_col:\n    train = train.drop(columns=dc)\n    test = test.drop(columns=dc)\n    for x in dc:\n        if x in ft:\n            ft.remove(x)\n\nneed_to_add = ['min_dist_feature_0', 'min_dist_feature_1', 'BN_binning', 'DU+FR', 'DA+DE', 'AB+GL', 'out_GL', 'DA*CS']\nfor x in need_to_add:\n    ft.append(x)\nprint(ft)\n    \nprint(train.columns.tolist())\nprint(train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.439442Z","iopub.execute_input":"2023-08-10T13:28:24.439829Z","iopub.status.idle":"2023-08-10T13:28:24.455479Z","shell.execute_reply.started":"2023-08-10T13:28:24.439795Z","shell.execute_reply":"2023-08-10T13:28:24.454290Z"},"trusted":true},"execution_count":161,"outputs":[{"name":"stdout","text":"['AF', 'BQ', 'AB', 'FL', 'CR', 'DH', 'DA', 'EH', 'CD ', 'BP', 'BC', 'DL', 'EE', 'DE', 'GL', 'FR', 'FI', 'EB', 'CU', 'BN', 'min_dist_feature_0', 'min_dist_feature_1', 'BN_binning', 'DU+FR', 'DA+DE', 'AB+GL', 'out_GL', 'DA*CS']\n['Id', 'AB', 'AF', 'AH', 'AM', 'AX', 'AY', 'AZ', 'BC', 'BN', 'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CR', 'CU', 'DA', 'DE', 'DF', 'DH', 'DL', 'DN', 'DY', 'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FE', 'FI', 'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL', 'Class', 'min_dist_feature_0', 'min_dist_feature_1', 'BN_binning', 'DU+FR', 'DA+DE', 'AB+GL', 'out_GL', 'DA*CS', 'poly_0_0', 'poly_0_1', 'poly_0_2', 'poly_0_3', 'poly_0_4']\n(617, 62)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 特征提取\npredictor_columns中提取上面所有除了Class和Id的属性特征。","metadata":{}},{"cell_type":"code","source":"predictor_columns = [n for n in train.columns if n != 'Class' and n != 'Id']","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.457174Z","iopub.execute_input":"2023-08-10T13:28:24.457917Z","iopub.status.idle":"2023-08-10T13:28:24.466729Z","shell.execute_reply.started":"2023-08-10T13:28:24.457882Z","shell.execute_reply":"2023-08-10T13:28:24.465767Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"markdown","source":"我们可以将greeks.csv中的Epsilon，也就是时间，加入到train dataset中。","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\ntimes = greeks.Epsilon.copy()\ntimes[greeks.Epsilon != 'Unknown'] = greeks.Epsilon[greeks.Epsilon != 'Unknown'].map(lambda x: datetime.strptime(x,'%m/%d/%Y').toordinal())\ntimes[greeks.Epsilon == 'Unknown'] = np.nan\ntimes = times.astype(np.float64)\n\ntrain_pred_and_time = pd.concat((train, times, greeks.Alpha), axis=1)\ntrain_cate = train_pred_and_time.iloc[:, -1]        # A, B, D, G\ntrain_pred_and_time = train_pred_and_time.drop(train_pred_and_time.columns[-1], axis=1)\n\ntest_predictors = test[predictor_columns]\ntest_time = np.zeros((len(test_predictors), 1)) + train_pred_and_time.Epsilon.max() + 1\ntest_pred_and_time = pd.concat((test_predictors, pd.DataFrame(test_time, columns=['Epsilon'])), axis=1)\n# test_pred_and_time = train_pred_and_time.copy().drop(['Id', 'Class'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.470246Z","iopub.execute_input":"2023-08-10T13:28:24.471745Z","iopub.status.idle":"2023-08-10T13:28:24.496818Z","shell.execute_reply.started":"2023-08-10T13:28:24.471711Z","shell.execute_reply":"2023-08-10T13:28:24.495642Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"markdown","source":"## Model & Evaluation","metadata":{}},{"cell_type":"markdown","source":"首先设置评判标准。在这次比赛中使用的评判标准是balanced log loss，公式如下：\n$$\n\\text { Log Loss }=\\frac{-\\frac{1}{N_0} \\sum_{i=1}^{N_0} y_{0 i} \\log p_{0 i}-\\frac{1}{N_1} \\sum_{i=1}^{N_1} y_{1 i} \\log p_{1 i}}{2}\n$$\n这样的目标是平衡两类的重要程度。","metadata":{"papermill":{"duration":0.010642,"end_time":"2023-07-14T18:25:42.008765","exception":false,"start_time":"2023-07-14T18:25:41.998123","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.metrics import log_loss\ndef balanced_log_loss(y_true, y_pred):\n    nc = np.bincount(y_true)\n    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15, labels=[0, 1])\n\ndef meta_to_bll(y_true, y_pred):\n    y_pred = y_pred.reshape(-1, 4)\n    y_true = np.array([0 if x == 0 else 1 for x in y_true])\n    probabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)\n    p0 = probabilities[:, :1]       # 计算class=0的概率\n    p1 = 1 - p0\n\n    loss = balanced_log_loss(y_true, p1)\n    return \"bll\", loss, False\n\ndef err(y_true, y_pred):\n#     print(np.concatenate((y_true, y_pred), axis=1))\n#     print(y_true, y_pred)\n    return abs(y_true - y_pred)","metadata":{"papermill":{"duration":0.020888,"end_time":"2023-07-14T18:25:42.040516","exception":false,"start_time":"2023-07-14T18:25:42.019628","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-10T13:28:24.498658Z","iopub.execute_input":"2023-08-10T13:28:24.499834Z","iopub.status.idle":"2023-08-10T13:28:24.510043Z","shell.execute_reply.started":"2023-08-10T13:28:24.499770Z","shell.execute_reply":"2023-08-10T13:28:24.509126Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"y_true = np.array([0, 0, 1, 1])\ny_pred = np.array([0.035, 0.035, 0.965, 0.965])\nprint(balanced_log_loss(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.512561Z","iopub.execute_input":"2023-08-10T13:28:24.513216Z","iopub.status.idle":"2023-08-10T13:28:24.527408Z","shell.execute_reply.started":"2023-08-10T13:28:24.513180Z","shell.execute_reply":"2023-08-10T13:28:24.526297Z"},"trusted":true},"execution_count":165,"outputs":[{"name":"stdout","text":"0.03562717764315116\n","output_type":"stream"}]},{"cell_type":"markdown","source":"然后设计集成模型，这里使用了4个分类器，两个XGBoost，两个TabPFN。","metadata":{"papermill":{"duration":0.010694,"end_time":"2023-07-14T18:25:42.274675","exception":false,"start_time":"2023-07-14T18:25:42.263981","status":"completed"},"tags":[]}},{"cell_type":"code","source":"XGB_params1 =  {'lambda': 0.0002570549301993319, 'alpha': 0.0017687267794924669, 'max_depth': 4, 'learning_rate': 0.09979383709722141, 'gamma': 0.07220786000357873, 'colsample_bytree': 0.939232633292054, 'min_child_weight': 2, 'subsample': 0.7443351925177553}\nXGB_params2 =  {'lambda': 0.0003741769532691957, 'alpha': 0.00027386497304802976, 'max_depth': 9, 'learning_rate': 0.09571794639204703, 'gamma': 1.2542733834438399e-05, 'colsample_bytree': 0.5362641371247696, 'min_child_weight': 3, 'subsample': 0.11567163589480782}\n# XGB_params2 =  {'lambda': 2.312149530300647e-06, 'alpha': 8.44800888247485e-06, 'max_depth': 9, 'learning_rate': 0.0606469701340082, 'gamma': 4.135034256351954e-08, 'colsample_bytree': 0.4998641476146233, 'min_child_weight': 3, 'subsample': 0.13052486707638683}\nLGBM_params1 = {'lambda_l1': 0.013086657734924316, 'lambda_l2': 0.021818112097617245, 'num_leaves': 7, 'learning_rate': 0.17939656611410498, 'feature_fraction': 0.6470978553425908, 'bagging_fraction': 0.9259314688300568, 'bagging_freq': 1, 'min_child_samples': 98}\nLGBM_params2 = {'lambda_l1': 2.125504936636207e-07, 'lambda_l2': 0.0004897340091318311, 'num_leaves': 8, 'learning_rate': 0.053376510932806945, 'feature_fraction': 0.4748425486495317, 'bagging_fraction': 0.38288586743438613, 'bagging_freq': 4, 'min_child_samples': 68}\nLGBM_params3 = {'colsample_bytree': 0.41751822001010247,\n                'learning_rate': 0.059510850084881564,\n                'max_depth': 10,\n                'min_child_samples': 14,\n                'num_leaves': 255,\n                'reg_alpha': 0.05762416020785592,\n                'reg_lambda': 0.03489804289769388,\n                'subsample': 0.6517804799892094,\n                }\nLGBM_params4 = {\n        'objective': 'binary', \n        'metric': 'binary_logloss', \n        'boosting': 'goss',\n        'learning_rate': 0.09110460114828077,\n        'num_leaves': 8,\n        'feature_fraction': 0.4989639912997521,\n        'bagging_fraction': 0.54872439795985,\n        'lambda_l1': 1.4522184914523175, \n        'lambda_l2': 1.7873553090132748e-08,\n        'is_unbalance':True, \n        'seed': 42,\n    }\nLGBM_params_13 = {\n    \"max_depth\": 4,\n    \"num_leaves\": 9,\n    \"min_child_samples\": 17,\n    \"n_estimators\": 200,\n    \"learning_rate\": 0.15,\n    \"colsample_bytree\": 0.4,\n    \"min_split_gain\": 1e-4,\n    \"reg_alpha\": 1e-2,\n    \"reg_lambda\": 5e-3,\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.529236Z","iopub.execute_input":"2023-08-10T13:28:24.529654Z","iopub.status.idle":"2023-08-10T13:28:24.543490Z","shell.execute_reply.started":"2023-08-10T13:28:24.529620Z","shell.execute_reply":"2023-08-10T13:28:24.542650Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"xgb_optuna1 = {\n    'n_estimators': 2000,\n    'learning_rate': 0.09641232707445854,\n    'booster': 'gbtree',\n    'lambda': 4.666002223704784,\n    'alpha': 3.708175990751336,\n    'subsample': 0.6100174145229473,\n    'colsample_bytree': 0.5506821152321051,\n    'max_depth': 7,\n    'min_child_weight': 3,\n    'eta': 1.740374368661041,\n    'gamma': 0.007427363662926455,\n    'grow_policy': 'depthwise',\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'verbosity': 0,\n    'random_state': 666,\n}\n\nxgb_optuna2 = {\n    'n_estimators': 2000,\n    'learning_rate': 0.012208383405206188,\n    'booster': 'gbtree',\n    'lambda': 0.009968756668882757,\n    'alpha': 0.02666266827121168,\n    'subsample': 0.7097814108897231,\n    'colsample_bytree': 0.7946945784285216,\n    'max_depth': 3,\n    'min_child_weight': 4,\n    'eta': 0.5480204506554545,\n    'gamma': 0.8788654128774149,\n    'scale_pos_weight': 4.71,\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'verbosity': 0,\n    'random_state': 38,\n}\n\nxgb_params4 = {\n    'n_estimators': 2000,\n    'colsample_bytree': 0.8757972257439255,\n    'gamma': 0.11135738771999848,\n    'max_depth': 7,\n    'min_child_weight': 3,\n    'reg_alpha': 0.4833998914998038,\n    'reg_lambda': 0.006223568555619563,\n    'scale_pos_weight': 8,\n    'subsample': 0.7056434340275685,\n    'random_state': 424\n}\n\nlgbm_params_2_2 = { 'boosting_type': 'goss', \n                    'learning_rate': 0.06733232950390658, \n                    'n_estimators': 50000, \n                    'early_stopping_round': 300, \n                    'random_state': 810,\n                    'subsample': 0.6970532011679706,\n                    'colsample_bytree': 0.6055755840633003,\n                    'class_weight': 'balanced',\n                    'metric': 'none', \n                    'is_unbalance': True,\n                    'max_depth': 8\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.546610Z","iopub.execute_input":"2023-08-10T13:28:24.547800Z","iopub.status.idle":"2023-08-10T13:28:24.560384Z","shell.execute_reply.started":"2023-08-10T13:28:24.547771Z","shell.execute_reply":"2023-08-10T13:28:24.559308Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"from scipy.stats.mstats import gmean\n\nclass Ensemble():\n    def __init__(self):\n        self.imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n        self.imputer4Tab = SimpleImputer(missing_values=np.nan, strategy='median')\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.classifiers = [\n            # xgboost\n            xgboost.XGBClassifier(**XGB_params1, n_estimators=2000),\n            xgboost.XGBClassifier(**XGB_params2, n_estimators=2000),\n#             xgboost.XGBClassifier(nestimators=2000),\n#             xgboost.XGBClassifier(colsample_bytree=0.8374643164834153, gamma=0.1, learning_rate=0.16064608298859606, max_depth=7, min_child_weight=1, n_estimators=500, reg_alpha=0.5, subsample=0.6687393925308378, random_state=19),\n            \n            # lightGBM\n            lgb.LGBMClassifier(**LGBM_params1,n_estimators=3000, early_stopping_rounds=100, verbose=-1),\n#             lgb.LGBMClassifier(**LGBM_params2,n_estimators=3000, early_stopping_rounds=100, verbose=-1),\n#             lgb.LGBMClassifier(**LGBM_params3,boosting_type='goss', random_state=42,class_weight='balanced',verbose=-1,n_estimators = 10000,early_stopping_rounds=300),\n            lgb.LGBMClassifier(**LGBM_params_13),\n#             lgb.LGBMClassifier(**LGBM_params_13),\n            \n            # TabPFN\n            TabPFNClassifier(N_ensemble_configurations=64, device=self.device),\n#             TabPFNClassifier(N_ensemble_configurations=128, device=self.device),\n            TabPFNClassifier(N_ensemble_configurations=256, device=self.device),\n\n        ]\n        self.classifiers_2 = [\n            lgb.LGBMClassifier(**LGBM_params4, n_estimators=3000, early_stopping_rounds=100, verbose=-1),\n            lgb.LGBMClassifier(**lgbm_params_2_2),\n            \n            CatBoostClassifier(verbose=0,),\n            \n            xgboost.XGBClassifier(**xgb_optuna1),\n            xgboost.XGBClassifier(**xgb_optuna2),\n#             xgboost.XGBClassifier(**xgb_params4),\n            \n            TabPFNClassifier(N_ensemble_configurations=32, device=self.device),\n        ]\n    \n    def fit(self, X, X4Tab, y, x_val, y_val_meta):\n        y = y.values\n        unique_classes, y = np.unique(y, return_inverse=True)\n        self.classes_ = unique_classes\n#         print(self.classes_)\n        X = self.imputer.fit_transform(X)\n        X4Tab = self.imputer4Tab.fit_transform(X4Tab)\n        \n        x_val = x_val.values\n        y_val_meta = y_val_meta.values\n        y_val_meta = np.searchsorted(self.classes_, y_val_meta)\n        for classifier in self.classifiers:\n            # if classifier != self.classifiers[0] and classifier != self.classifiers[1]:\n            if isinstance(classifier, TabPFNClassifier):\n                classifier.fit(X4Tab, y, overwrite_warning =True)\n            elif isinstance(classifier, lgb.LGBMClassifier):\n                classifier.fit(X, y, eval_set=[(x_val, y_val_meta)], \n                               eval_metric='multi_logloss',\n                               verbose=10000,\n#                                 eval_metric=meta_to_bll,\n#                                callbacks=[empty_callback],\n                               )\n            elif isinstance(classifier, xgboost.XGBClassifier):\n                classifier.fit(X, y, eval_set=[(x_val, y_val_meta)], early_stopping_rounds=100, verbose=False)\n            else :\n                classifier.fit(X, y)\n        \n        y_2 = y.copy()\n        y_val_meta_2 = y_val_meta.copy()\n        y_2[y_2 > 0] = 1\n        y_val_meta_2[y_val_meta_2 > 0] = 1\n        for classifier in self.classifiers_2:\n            if isinstance(classifier, TabPFNClassifier):\n                classifier.fit(X, y_2, overwrite_warning =True)\n            elif isinstance(classifier, lgb.LGBMClassifier):\n                classifier.fit(X, y_2, eval_set=[(x_val, y_val_meta_2)], \n                               eval_metric='logloss',\n                               verbose=10000,\n                               )\n            elif isinstance(classifier, xgboost.XGBClassifier):\n                classifier.fit(X, y_2, eval_set=[(x_val, y_val_meta_2)], early_stopping_rounds=100, verbose=False)\n            else :\n                classifier.fit(X, y_2)\n    \n    def predict_proba(self, x, x4Tab):\n        x = self.imputer.transform(x)\n        x4Tab = self.imputer4Tab.transform(x4Tab)\n        \n        # As for 4 categories, we squeeze into 2 firstly\n        probabilities = np.stack([classifier.predict_proba(x4Tab) if isinstance(classifier, TabPFNClassifier) else classifier.predict_proba(x) for classifier in self.classifiers])\n        probabilities[:, :, 1] = probabilities[:, :, 1:].sum(axis=2)   # calc sum to binary results\n        probabilities = probabilities[:, :, :2].copy()\n        \n        probabilities_2 = np.stack([classifier.predict_proba(x4Tab) if isinstance(classifier, TabPFNClassifier) else classifier.predict_proba(x) for classifier in self.classifiers_2])\n        probabilities = np.concatenate((probabilities, probabilities_2), axis=0)\n        \n        if Post_Process['Ensemble_method'] == 'mean':\n            averaged_probabilities = np.mean(probabilities, axis=0)\n        elif Post_Process['Ensemble_method'] == 'gmean':\n            averaged_probabilities = gmean(probabilities, axis=0)\n            \n        class_0_est_instances = averaged_probabilities[:, 0].sum()\n        others_est_instances = averaged_probabilities[:, 1:].sum()\n        \n        # Weighted probabilities based on class imbalance(default 1:1)\n        new_probabilities = averaged_probabilities\n        if Post_Process['trival_calib'] == True:\n            new_probabilities = averaged_probabilities * np.array([[(0.58/class_0_est_instances if i==0 else 0.42/others_est_instances) for i in range(averaged_probabilities.shape[1])]])\n    \n        odds = Post_Process[\"boost\"] * new_probabilities[:, 0] / (1 - new_probabilities[:, 0])\n        new_probabilities[:, 0] = odds / (1 + odds)\n        \n        new_probabilities = new_probabilities / np.sum(new_probabilities, axis=1, keepdims=1) \n        \n        return new_probabilities","metadata":{"papermill":{"duration":0.029577,"end_time":"2023-07-14T18:25:42.315244","exception":false,"start_time":"2023-07-14T18:25:42.285667","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-10T13:28:24.564709Z","iopub.execute_input":"2023-08-10T13:28:24.565073Z","iopub.status.idle":"2023-08-10T13:28:24.593210Z","shell.execute_reply.started":"2023-08-10T13:28:24.565040Z","shell.execute_reply":"2023-08-10T13:28:24.592037Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"markdown","source":"这里我们设置了两个KFold，一个为outer，用于选80%的trainning dataset和20%的validation dataset；\n一个为inner，用于对trainning dataset分成5折，分别训练出5个模型（5-折模型）。用5-折模型对outer分出的20%的validation dataset预测并计算balanced log loss。\n最后选取效果最好的5-折模型对test预测（即分别用5个模型预测，取均值）","metadata":{"papermill":{"duration":0.010486,"end_time":"2023-07-14T18:25:42.336775","exception":false,"start_time":"2023-07-14T18:25:42.326289","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import KFold as KF, GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold as SKF\n\n\ncv_outer = SKF(n_splits = 8, shuffle=True, random_state=20230806)\ncv_inner = KF(n_splits = 5, shuffle=True, random_state=19)","metadata":{"papermill":{"duration":0.021079,"end_time":"2023-07-14T18:25:42.368643","exception":false,"start_time":"2023-07-14T18:25:42.347564","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-10T13:28:24.594947Z","iopub.execute_input":"2023-08-10T13:28:24.595322Z","iopub.status.idle":"2023-08-10T13:28:24.607831Z","shell.execute_reply.started":"2023-08-10T13:28:24.595287Z","shell.execute_reply":"2023-08-10T13:28:24.606832Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"# 计算预测准确率\ndef calc_acc(y_pred, y):\n    probabilities = np.concatenate((y_pred[:, :1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)\n    p0 = probabilities[:, :1]       # 计算class=0的概率\n    p1 = 1 - p0\n    \n    y = y.values.astype(int)\n    cnt = 0\n\n    for i in range(len(p0)):\n        if p0[i] >= p1[i]:\n            lab = 0\n        else :\n            lab = 1\n\n        if lab == y[i]:\n            cnt += 1\n\n    return cnt / len(p0)\n\n# 计算balanced log loss\ndef calc_loss(y_pred, y):\n    probabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)\n    p0 = probabilities[:, :1]       # 计算class=\n    \n    if Post_Process[\"optimal\"] == True:\n        N = p0.sum()\n        M = probabilities.sum() - N\n#         print(M, N, y.shape)\n        p0 = (p0 * (M+1)) / (p0*(M-N) + N + 1)\n        p_non = probabilities[:, :1]\n    \n    \n    if Post_Process['Set_thres'] == True:\n        p0[(p0 > thres_high_l) & (p0 < thres_high_r)] = thres_high_r\n        p0[(p0 < thres_low_r) & (p0 > thres_low_l)] = thres_low_l\n\n    p1 = 1 - p0\n    \n    y = y.values.astype(int)\n    loss = balanced_log_loss(y, p1)\n\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.609513Z","iopub.execute_input":"2023-08-10T13:28:24.610079Z","iopub.status.idle":"2023-08-10T13:28:24.623272Z","shell.execute_reply.started":"2023-08-10T13:28:24.610042Z","shell.execute_reply":"2023-08-10T13:28:24.622065Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nros = RandomOverSampler(random_state=42)\n\ndef training(model, x, y, y_meta):\n    low_loss = np.inf\n    best_models = []\n    best_loss = []\n\n    for out_id, (train_idx, val_idx) in enumerate(cv_outer.split(x, y_meta), start=1):\n        print(f'Now for outer fold {out_id}:')\n        x_train_ori, x_val = x.iloc[train_idx], x.iloc[val_idx]\n        y_train_ori, y_val = y_meta.iloc[train_idx], y.iloc[val_idx]\n\n        x_train, y_train = ros.fit_resample(x_train_ori, y_train_ori)\n        # x_train, y_train = x_train_ori, y_train_ori\n        \n        train_loss = np.zeros((x_train.shape[0], 4))\n    \n        out_X, out_y_meta = x_train, y_train\n        out_y = out_y_meta.apply(lambda x: 0 if x == 'A' else 1)\n        \n        models = []\n        losses = []\n\n        for in_id, (train_idx1, val_idx1) in enumerate(cv_inner.split(out_X), start=1):\n            in_x_train, in_x_val = out_X.iloc[train_idx1], out_X.iloc[val_idx1]\n            in_y_train, in_y_val = out_y_meta.iloc[train_idx1], out_y.iloc[val_idx1]\n            in_y_val_meta = out_y_meta.iloc[val_idx1]\n\n            model.fit(in_x_train, in_y_train, in_x_val, in_y_val_meta)\n            models.append(model) \n\n            y_pred = model.predict_proba(in_x_val)\n            train_loss[val_idx1] = y_pred\n\n            metric = calc_loss(y_pred, in_y_val)\n            losses.append(metric)\n            print('Inner_fold = %.1f, val_loss = %.5f' % (in_id, metric))\n        \n        # 分别用models中的模型计算x_val的loss\n        val_y_pred = np.zeros((x_val.shape[0], 4))\n        for los_idx, model in enumerate(models):\n            y_pred = model.predict_proba(x_val)\n            val_y_pred += y_pred\n        val_y_pred /= len(models)\n\n        metric_train = calc_loss(train_loss, out_y)\n        acc_train = calc_acc(train_loss, out_y)\n        print(f'80% Train Loss: {metric_train}; Train Acc: {acc_train}')\n        metric_val = calc_loss(val_y_pred, y_val)\n        acc_val = calc_acc(val_y_pred, y_val)\n        print(f'20% Val Loss: {metric_val}; Val Acc: {acc_val}\\n')\n\n        if metric_val < low_loss:\n            low_loss = metric_val\n            best_models = models\n            best_loss = losses\n\n        # break       # 先只跑一次，节约时间    \n        \n    return best_models, best_loss","metadata":{"papermill":{"duration":0.133413,"end_time":"2023-07-14T18:25:42.547250","exception":false,"start_time":"2023-07-14T18:25:42.413837","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-10T13:28:24.625280Z","iopub.execute_input":"2023-08-10T13:28:24.625687Z","iopub.status.idle":"2023-08-10T13:28:24.641258Z","shell.execute_reply.started":"2023-08-10T13:28:24.625651Z","shell.execute_reply":"2023-08-10T13:28:24.640294Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"def training_kaggle(model, x, y, y_meta):\n    models = []\n    losses = []\n    errors = np.zeros((x.shape[0], ), dtype=np.float64)\n    train_preds = np.zeros((x.shape[0], 2), dtype=np.float64)\n\n    for out_id, (train_idx, val_idx) in enumerate(cv_outer.split(x, y_meta), start=1):\n        print(f'Now for fold {out_id}:')\n        x_train_ori, x_val = x.iloc[train_idx], x.iloc[val_idx]\n        y_train_ori, y_val = y_meta.iloc[train_idx], y.iloc[val_idx]\n        y_val_meta = y_meta.iloc[val_idx]\n        y_train_label = y.iloc[train_idx]\n\n        cols_x_train_ori = len(x_train_ori.columns)\n        y_train_ori_df = pd.DataFrame(y_train_ori, columns=['Alpha'])\n        x_train_ori_comb = pd.concat((x_train_ori, y_train_ori_df), axis=1)\n\n#         train_ros, y_nonsense = ros.fit_resample(x_train_ori_comb, y_train_label)    # 按 0/1 over sample\n#         x_train = train_ros.iloc[:, :cols_x_train_ori].copy()\n#         y_train = train_ros.iloc[:, cols_x_train_ori:].copy()\n        x_train, y_train = ros.fit_resample(x_train_ori, y_train_ori)         # 按 A/B/D/G over sample\n        # x_train, y_train = x_train_ori, y_train_ori\n\n        out_X, out_y_meta = x_train, y_train\n        # out_y = out_y_meta.apply(lambda x: 0 if x == 'A' else 1)\n        out_X4Tab = out_X.copy()         # try to extract the useful features for TabPFN\n        model.fit(out_X, out_X4Tab, out_y_meta, x_val, y_val_meta)\n        models.append(model)\n        \n        # 用训练好的model计算x_val的loss\n        x_val4Tab = x_val.copy()\n        val_y_pred = model.predict_proba(x_val, x_val4Tab) # try to extract the useful features for TabPFN\n        train_preds[val_idx] = val_y_pred\n        errors[val_idx] = err(y_val, val_y_pred[:, 1:].sum(axis=1))\n\n        metric_val = calc_loss(val_y_pred, y_val)\n        losses.append(metric_val)\n        acc_val = calc_acc(val_y_pred, y_val)\n        print(f'20% Val Loss: {metric_val}; Val Acc: {acc_val}\\n')\n\n        # break       # 先只跑一次，节约时间    \n        \n    return models, losses, errors, train_preds","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.648727Z","iopub.execute_input":"2023-08-10T13:28:24.651182Z","iopub.status.idle":"2023-08-10T13:28:24.665324Z","shell.execute_reply.started":"2023-08-10T13:28:24.651143Z","shell.execute_reply":"2023-08-10T13:28:24.664276Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"def training_loo(model, x, y, y_meta):\n    loo = LeaveOneOut()\n    \n    models = []\n    errors = []\n    train_preds = np.zeros((x.shape[0], 4), dtype=np.float64)\n\n    for train_idx, val_idx in tqdm(loo.split(x), total = x.shape[0]):\n        x_train_ori, x_val = x.iloc[train_idx], x.iloc[val_idx]\n        y_train_ori, y_val = y_meta.iloc[train_idx], y.iloc[val_idx]\n        y_val_meta = y_meta.iloc[val_idx]\n\n#         x_train, y_train = ros.fit_resample(x_train_ori, y_train_ori)\n        x_train, y_train = x_train_ori, y_train_ori\n    \n        model.fit(x_train, y_train, x_val, y_val_meta)\n        models.append(model)\n        \n        val_y_pred = model.predict_proba(x_val)\n        train_preds[val_idx] = val_y_pred\n        \n        errors.append(err(y_val, val_y_pred[:,1:].sum()))\n    \n    return models, errors, train_preds","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:24.667807Z","iopub.execute_input":"2023-08-10T13:28:24.668455Z","iopub.status.idle":"2023-08-10T13:28:24.681167Z","shell.execute_reply.started":"2023-08-10T13:28:24.668420Z","shell.execute_reply":"2023-08-10T13:28:24.680211Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"markdown","source":"用模型对验证集在本地进行指标评估(balanced log loss).","metadata":{}},{"cell_type":"code","source":"x_ = train_pred_and_time.drop(['Class', 'Id'], axis=1)\ny_ = train_pred_and_time['Class']\ny_meta_ = train_cate\n\nyt = Ensemble()\n\nprint(x_.shape, y_.shape)\n# models, losses = training(yt, x_, y_, y_meta_)\nmodels, losses, errors, train_preds = training_kaggle(yt, x_, y_, y_meta_)\n# models, errors, train_preds = training_loo(yt, x_, y_, y_meta_)","metadata":{"papermill":{"duration":1898.965442,"end_time":"2023-07-14T18:57:21.559803","exception":false,"start_time":"2023-07-14T18:25:42.594361","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-10T13:28:24.684623Z","iopub.execute_input":"2023-08-10T13:28:24.685128Z","iopub.status.idle":"2023-08-10T13:33:57.818098Z","shell.execute_reply.started":"2023-08-10T13:28:24.685103Z","shell.execute_reply":"2023-08-10T13:33:57.816833Z"},"trusted":true},"execution_count":174,"outputs":[{"name":"stdout","text":"Loading model that can be used for inference only\nUsing a Transformer with 25.82 M parameters\nLoading model that can be used for inference only\nUsing a Transformer with 25.82 M parameters\nLoading model that can be used for inference only\nUsing a Transformer with 25.82 M parameters\n(617, 61) (617,)\nNow for fold 1:\n[LightGBM] [Warning] bagging_fraction is set=0.9259314688300568, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259314688300568\n[LightGBM] [Warning] lambda_l1 is set=0.013086657734924316, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013086657734924316\n[LightGBM] [Warning] feature_fraction is set=0.6470978553425908, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6470978553425908\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] lambda_l2 is set=0.021818112097617245, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.021818112097617245\n20% Val Loss: 0.10343073104028024; Val Acc: 0.9743589743589743\n\nNow for fold 2:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] bagging_fraction is set=0.9259314688300568, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259314688300568\n[LightGBM] [Warning] lambda_l1 is set=0.013086657734924316, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013086657734924316\n[LightGBM] [Warning] feature_fraction is set=0.6470978553425908, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6470978553425908\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] lambda_l2 is set=0.021818112097617245, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.021818112097617245\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"20% Val Loss: 0.31943874994328153; Val Acc: 0.8831168831168831\n\nNow for fold 3:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] bagging_fraction is set=0.9259314688300568, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259314688300568\n[LightGBM] [Warning] lambda_l1 is set=0.013086657734924316, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013086657734924316\n[LightGBM] [Warning] feature_fraction is set=0.6470978553425908, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6470978553425908\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] lambda_l2 is set=0.021818112097617245, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.021818112097617245\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"20% Val Loss: 0.185349739138346; Val Acc: 0.935064935064935\n\nNow for fold 4:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] bagging_fraction is set=0.9259314688300568, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259314688300568\n[LightGBM] [Warning] lambda_l1 is set=0.013086657734924316, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013086657734924316\n[LightGBM] [Warning] feature_fraction is set=0.6470978553425908, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6470978553425908\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] lambda_l2 is set=0.021818112097617245, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.021818112097617245\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"20% Val Loss: 0.07878315922568181; Val Acc: 0.961038961038961\n\nNow for fold 5:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] bagging_fraction is set=0.9259314688300568, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259314688300568\n[LightGBM] [Warning] lambda_l1 is set=0.013086657734924316, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013086657734924316\n[LightGBM] [Warning] feature_fraction is set=0.6470978553425908, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6470978553425908\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] lambda_l2 is set=0.021818112097617245, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.021818112097617245\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"20% Val Loss: 0.13817745545294338; Val Acc: 0.961038961038961\n\nNow for fold 6:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] bagging_fraction is set=0.9259314688300568, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259314688300568\n[LightGBM] [Warning] lambda_l1 is set=0.013086657734924316, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013086657734924316\n[LightGBM] [Warning] feature_fraction is set=0.6470978553425908, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6470978553425908\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] lambda_l2 is set=0.021818112097617245, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.021818112097617245\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"20% Val Loss: 0.19042126322181466; Val Acc: 0.8961038961038961\n\nNow for fold 7:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] bagging_fraction is set=0.9259314688300568, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259314688300568\n[LightGBM] [Warning] lambda_l1 is set=0.013086657734924316, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013086657734924316\n[LightGBM] [Warning] feature_fraction is set=0.6470978553425908, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6470978553425908\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] lambda_l2 is set=0.021818112097617245, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.021818112097617245\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"20% Val Loss: 0.11222276237234641; Val Acc: 0.961038961038961\n\nNow for fold 8:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] bagging_fraction is set=0.9259314688300568, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259314688300568\n[LightGBM] [Warning] lambda_l1 is set=0.013086657734924316, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013086657734924316\n[LightGBM] [Warning] feature_fraction is set=0.6470978553425908, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6470978553425908\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] lambda_l2 is set=0.021818112097617245, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.021818112097617245\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"20% Val Loss: 0.13046371600959747; Val Acc: 0.935064935064935\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"计算一下对训练集的预测的概率的loss，以及使用probability calibration后的loss.\n这里的probability calibration我们尝试了Platt Scale & Isotionic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.isotonic import IsotonicRegression\n# from betacal import BetaCalibration\n\ntrain_label = train_pred_and_time['Class']\n\n# before calibration\nprint('Before calibration:')\nprint(f'Bll: {calc_loss(train_preds, train_label)}, Acc: {calc_acc(train_preds, train_label)}\\n')\n\n# after calibration\nprint('After calibration:')\ntrain_preds2 = train_preds[:, 1:].sum(axis=1).reshape(-1, 1)\nros_pc = RandomOverSampler(random_state=38)\ntrain_preds2_ros, train_label_ros = ros_pc.fit_resample(train_preds2, train_label)\n\nif Post_Process['Calibration'] == 'bc':\n    Calib = BetaCalibration(parameters=\"abm\")\n    Calib.fit(train_preds2_ros, train_label_ros)\n    print(f'Bll: {balanced_log_loss(train_label, Calib.predict(train_preds2))}\\n')\nelif Post_Process['Calibration'] == 'iso':\n    Calib = IsotonicRegression(out_of_bounds='clip')\n    Calib.fit(train_preds2_ros, train_label_ros)\n    print(f'Bll: {balanced_log_loss(train_label, Calib.predict(train_preds2))}\\n')\n\n# print(np.concatenate((train_preds2, train_label.values.reshape(-1, 1)), axis=1))","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:33:57.819727Z","iopub.execute_input":"2023-08-10T13:33:57.820362Z","iopub.status.idle":"2023-08-10T13:33:57.840743Z","shell.execute_reply.started":"2023-08-10T13:33:57.820329Z","shell.execute_reply":"2023-08-10T13:33:57.839678Z"},"trusted":true},"execution_count":175,"outputs":[{"name":"stdout","text":"Before calibration:\nBll: 0.1567333645748055, Acc: 0.9384116693679092\n\nAfter calibration:\n","output_type":"stream"}]},{"cell_type":"code","source":"errors = pd.DataFrame(errors)\nerrors.join(greeks.Alpha).sort_values(0).tail(30)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:33:57.842735Z","iopub.execute_input":"2023-08-10T13:33:57.843159Z","iopub.status.idle":"2023-08-10T13:33:57.863483Z","shell.execute_reply.started":"2023-08-10T13:33:57.843118Z","shell.execute_reply":"2023-08-10T13:33:57.862299Z"},"trusted":true},"execution_count":176,"outputs":[{"execution_count":176,"output_type":"execute_result","data":{"text/plain":"            0 Alpha\n203  0.583941     A\n503  0.592838     A\n146  0.596212     B\n61   0.610563     A\n337  0.612380     A\n521  0.612414     A\n322  0.614230     A\n55   0.619037     A\n220  0.622440     A\n313  0.636637     B\n193  0.658304     B\n553  0.691263     A\n360  0.695455     A\n498  0.699698     A\n356  0.738213     A\n434  0.741273     B\n556  0.749667     A\n468  0.779196     A\n380  0.786282     A\n194  0.786307     A\n458  0.813847     A\n514  0.815118     A\n292  0.824456     A\n367  0.853194     A\n462  0.868948     A\n586  0.903962     A\n231  0.909327     A\n190  0.929019     A\n102  0.950110     A\n509  0.993670     D","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>Alpha</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>203</th>\n      <td>0.583941</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>0.592838</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.596212</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>0.610563</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>337</th>\n      <td>0.612380</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>521</th>\n      <td>0.612414</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>322</th>\n      <td>0.614230</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>0.619037</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>220</th>\n      <td>0.622440</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>313</th>\n      <td>0.636637</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>0.658304</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>553</th>\n      <td>0.691263</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>360</th>\n      <td>0.695455</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>0.699698</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>356</th>\n      <td>0.738213</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>434</th>\n      <td>0.741273</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>556</th>\n      <td>0.749667</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>468</th>\n      <td>0.779196</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>0.786282</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>0.786307</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>458</th>\n      <td>0.813847</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>514</th>\n      <td>0.815118</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>292</th>\n      <td>0.824456</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>367</th>\n      <td>0.853194</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>462</th>\n      <td>0.868948</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>586</th>\n      <td>0.903962</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>231</th>\n      <td>0.909327</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>0.929019</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>0.950110</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>509</th>\n      <td>0.993670</td>\n      <td>D</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"errors.join(greeks.Alpha).groupby('Alpha').agg(['mean', 'std', 'max'])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:33:57.867222Z","iopub.execute_input":"2023-08-10T13:33:57.867623Z","iopub.status.idle":"2023-08-10T13:33:57.890665Z","shell.execute_reply.started":"2023-08-10T13:33:57.867589Z","shell.execute_reply":"2023-08-10T13:33:57.886612Z"},"trusted":true},"execution_count":177,"outputs":[{"execution_count":177,"output_type":"execute_result","data":{"text/plain":"              0                    \n           mean       std       max\nAlpha                              \nA      0.095126  0.183645  0.950110\nB      0.106027  0.168320  0.741273\nD      0.176148  0.254974  0.993670\nG      0.061872  0.108515  0.509798","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"3\" halign=\"left\">0</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>mean</th>\n      <th>std</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>Alpha</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>A</th>\n      <td>0.095126</td>\n      <td>0.183645</td>\n      <td>0.950110</td>\n    </tr>\n    <tr>\n      <th>B</th>\n      <td>0.106027</td>\n      <td>0.168320</td>\n      <td>0.741273</td>\n    </tr>\n    <tr>\n      <th>D</th>\n      <td>0.176148</td>\n      <td>0.254974</td>\n      <td>0.993670</td>\n    </tr>\n    <tr>\n      <th>G</th>\n      <td>0.061872</td>\n      <td>0.108515</td>\n      <td>0.509798</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_pred_and_time4Tab = test_pred_and_time.copy()\ny_pred = np.zeros((test_pred_and_time.shape[0], 2))\n# print(len(models))\nfor los_idx, model in enumerate(models):\n    y_pred += model.predict_proba(test_pred_and_time, test_pred_and_time4Tab) / losses[los_idx]\ny_pred = y_pred / y_pred[:, :].sum(axis=1, keepdims=True)\n\nprint(f\"Weight: {[(1/x) for x in losses]}\")\n\nprobabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:,1:], 1, keepdims=True)), axis=1)\np0 = probabilities[:, :1]\np1 = 1 - p0\np0 = p0.astype(np.float64)\n\nif Post_Process[\"optimal\"] == True:\n    N = p0.sum()\n    M = probabilities.sum() - N\n#         print(M, N, y.shape)\n    p0 = (p0 * (M+1)) / (p0*(M-N) + N + 1)\n    p_non = probabilities[:, :1]\n\nif Post_Process['Calibration'] != 'none':\n    p1 = Calib.predict(p1)\n    p0 = 1 - p1\n    \nif Post_Process['Set_thres'] == True:\n    p0[p0 > thres_high_l & p0 < thres_high_r] = thres_high_r\n    p0[p0 < thres_low_r & po > thres_low_l] = thres_low_l","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:33:57.892396Z","iopub.execute_input":"2023-08-10T13:33:57.893241Z","iopub.status.idle":"2023-08-10T13:35:48.094947Z","shell.execute_reply.started":"2023-08-10T13:33:57.893193Z","shell.execute_reply":"2023-08-10T13:35:48.093343Z"},"trusted":true},"execution_count":178,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[178], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(len(models))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m los_idx, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n\u001b[0;32m----> 5\u001b[0m     y_pred \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pred_and_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pred_and_time4Tab\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m losses[los_idx]\n\u001b[1;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred \u001b[38;5;241m/\u001b[39m y_pred[:, :]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mx)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mlosses]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[168], line 94\u001b[0m, in \u001b[0;36mEnsemble.predict_proba\u001b[0;34m(self, x, x4Tab)\u001b[0m\n\u001b[1;32m     91\u001b[0m probabilities[:, :, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m probabilities[:, :, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)   \u001b[38;5;66;03m# calc sum to binary results\u001b[39;00m\n\u001b[1;32m     92\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m probabilities[:, :, :\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 94\u001b[0m probabilities_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([classifier\u001b[38;5;241m.\u001b[39mpredict_proba(x4Tab) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(classifier, TabPFNClassifier) \u001b[38;5;28;01melse\u001b[39;00m classifier\u001b[38;5;241m.\u001b[39mpredict_proba(x) \u001b[38;5;28;01mfor\u001b[39;00m classifier \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifiers_2])\n\u001b[1;32m     95\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((probabilities, probabilities_2), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Post_Process[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnsemble_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n","Cell \u001b[0;32mIn[168], line 94\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     91\u001b[0m probabilities[:, :, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m probabilities[:, :, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)   \u001b[38;5;66;03m# calc sum to binary results\u001b[39;00m\n\u001b[1;32m     92\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m probabilities[:, :, :\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 94\u001b[0m probabilities_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx4Tab\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(classifier, TabPFNClassifier) \u001b[38;5;28;01melse\u001b[39;00m classifier\u001b[38;5;241m.\u001b[39mpredict_proba(x) \u001b[38;5;28;01mfor\u001b[39;00m classifier \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifiers_2])\n\u001b[1;32m     95\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((probabilities, probabilities_2), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Post_Process[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnsemble_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tabpfn/scripts/transformer_prediction_interface.py:284\u001b[0m, in \u001b[0;36mTabPFNClassifier.predict_proba\u001b[0;34m(self, X, normalize_with_test, return_logits)\u001b[0m\n\u001b[1;32m    266\u001b[0m prediction \u001b[38;5;241m=\u001b[39m transformer_predict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m2\u001b[39m], X_full, y_full, eval_pos,\n\u001b[1;32m    267\u001b[0m                                  device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    268\u001b[0m                                  style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m                                  batch_size_inference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size_inference,\n\u001b[1;32m    281\u001b[0m                                  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mget_params_from_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc))\n\u001b[1;32m    282\u001b[0m prediction_, y_ \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), y_full\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()[eval_pos:]\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprediction_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad \u001b[38;5;28;01melse\u001b[39;00m prediction_\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"submission = pd.DataFrame(test[\"Id\"], columns=[\"Id\"])\nsubmission[\"class_0\"] = p0\nsubmission[\"class_1\"] = 1.0 - p0\nsubmission.to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.029952,"end_time":"2023-07-14T19:11:03.420860","exception":false,"start_time":"2023-07-14T19:11:03.390908","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-10T13:35:48.096433Z","iopub.status.idle":"2023-08-10T13:35:48.097345Z","shell.execute_reply.started":"2023-08-10T13:35:48.097081Z","shell.execute_reply":"2023-08-10T13:35:48.097107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df = pd.read_csv('submission.csv')\nprint(submission.head(20))","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:35:48.099867Z","iopub.status.idle":"2023-08-10T13:35:48.100666Z","shell.execute_reply.started":"2023-08-10T13:35:48.100391Z","shell.execute_reply":"2023-08-10T13:35:48.100418Z"},"trusted":true},"execution_count":null,"outputs":[]}]}