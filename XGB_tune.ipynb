{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,normalize\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import xgboost\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "from tabpfn import TabPFNClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./icr-identify-age-related-conditions/train.csv')\n",
    "test = pd.read_csv('./icr-identify-age-related-conditions/test.csv')\n",
    "sample = pd.read_csv('./icr-identify-age-related-conditions/sample_submission.csv')\n",
    "greeks = pd.read_csv('./icr-identify-age-related-conditions/greeks.csv')\n",
    "\n",
    "first_category = train.EJ.unique()[0]\n",
    "train.EJ = train.EJ.eq(first_category).astype('int')\n",
    "test.EJ = test.EJ.eq(first_category).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_denominators = {\n",
    "    'AB': 0.004273,\n",
    "    'AF': 0.00242,\n",
    "    'AH': 0.008709,\n",
    "    'AM': 0.003097,\n",
    "    'AR': 0.005244,\n",
    "    'AX': 0.008859,\n",
    "    'AY': 0.000609,\n",
    "    'AZ': 0.006302,\n",
    "    'BC': 0.007028,\n",
    "    'BD ': 0.00799,\n",
    "    'BN': 0.3531,\n",
    "    'BP': 0.004239,\n",
    "    'BQ': 0.002605,\n",
    "    'BR': 0.006049,\n",
    "    'BZ': 0.004267,\n",
    "    'CB': 0.009191,\n",
    "    'CC': 6.12e-06,\n",
    "    'CD ': 0.007928,\n",
    "    'CF': 0.003041,\n",
    "    'CH': 0.000398,\n",
    "    'CL': 0.006365,\n",
    "    'CR': 7.5e-05,\n",
    "    'CS': 0.003487,\n",
    "    'CU': 0.005517,\n",
    "    'CW ': 9.2e-05,\n",
    "    'DA': 0.00388,\n",
    "    'DE': 0.004435,\n",
    "    'DF': 0.000351,\n",
    "    'DH': 0.002733,\n",
    "    'DI': 0.003765,\n",
    "    'DL': 0.00212,\n",
    "    'DN': 0.003412,\n",
    "    'DU': 0.0013794,\n",
    "    'DV': 0.00259,\n",
    "    'DY': 0.004492,\n",
    "    'EB': 0.007068,\n",
    "    'EE': 0.004031,\n",
    "    'EG': 0.006025,\n",
    "    'EH': 0.006084,\n",
    "    'EL': 0.000429,\n",
    "    'EP': 0.009269,\n",
    "    'EU': 0.005064,\n",
    "    'FC': 0.005712,\n",
    "    'FD ': 0.005937,\n",
    "    'FE': 0.007486,\n",
    "    'FI': 0.005513,\n",
    "    'FR': 0.00058,\n",
    "    'FS': 0.006773,\n",
    "    'GB': 0.009302,\n",
    "    'GE': 0.004417,\n",
    "    'GF': 0.004374,\n",
    "    'GH': 0.003721,\n",
    "    'GI': 0.002572\n",
    "}\n",
    "for k, v in int_denominators.items():\n",
    "    train[k] = np.round(train[k] / v, 1)\n",
    "    test[k] = np.round(test[k] / v, 1)\n",
    "\n",
    "chose_col = ['Id','AF','BQ','AB','DU','DI','FL','CR','DH','BN','DA','EH','CD ','BP', 'DL','EE','FD ','DE','GL','FR','FI','EB','CU','CS', 'BR', 'BZ', 'CC']\n",
    "train = train[chose_col + ['Class']]\n",
    "test = test[chose_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "columns_to_select = [col for col in train.columns if col not in ['Class', 'Id']]\n",
    "\n",
    "train_data = train[columns_to_select].copy()\n",
    "test_data = test[columns_to_select].copy()\n",
    "\n",
    "# 填充缺失值\n",
    "train_data = pd.DataFrame(Imp.fit_transform(train_data), columns=columns_to_select)\n",
    "test_data = pd.DataFrame(Imp.transform(test_data), columns=columns_to_select)\n",
    "\n",
    "# 重新组合数据和原始列\n",
    "train_filled = pd.concat([train['Id'], train_data, train['Class']], axis=1)\n",
    "test_filled = pd.concat([test['Id'], test_data], axis=1)\n",
    "\n",
    "train = train_filled.copy()\n",
    "test = test_filled.copy()\n",
    "# print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN_binning\n",
      "5    159\n",
      "3    147\n",
      "4    140\n",
      "2     78\n",
      "6     66\n",
      "1     27\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 7\n",
    "BNpd = train['BN']\n",
    "\n",
    "BNpd = pd.concat([train['BN'], test['BN']], axis=0, ignore_index=True)\n",
    "data = BNpd.values.reshape(-1, 1)\n",
    "kmodel = KMeans(n_clusters=k)           # k为聚成几类\n",
    "kmodel.fit(data)  # 训练模型\n",
    "c = pd.DataFrame(kmodel.cluster_centers_, columns=['cc']) # 求聚类中心\n",
    "c0 = pd.DataFrame({'cc': [0.0]})\n",
    "c = pd.concat([c0, c], axis=0, ignore_index=True)\n",
    "c = c.sort_values(by='cc').reset_index(drop=True)\n",
    "\n",
    "# 求聚类中心之间的平均值作为分割点\n",
    "for i in range(c.shape[0] - 1):\n",
    "    c.iloc[i]['cc'] = (c.iloc[i]['cc'] + c.iloc[i+1]['cc']) / 2\n",
    "c = c.drop(c.index[-1])\n",
    "\n",
    "c0 = pd.DataFrame({'cc': [0.0]})\n",
    "cn = pd.DataFrame({'cc': [max(train['BN'].max(), test['BN'].max()) * 5]})\n",
    "c = pd.concat([c0, c, cn], axis=0, ignore_index=True)\n",
    "c = c['cc'].round().astype(int)\n",
    "c = c.unique()\n",
    "range_num = c.shape[0] - 1\n",
    "c = c.tolist()\n",
    "\n",
    "# 保留旧BN，添加BN_binning\n",
    "train_BN = train['BN'].values\n",
    "train_binning = pd.cut(train_BN, c, labels=range(range_num), include_lowest=True).astype(int)\n",
    "train['BN_binning'] = train_binning\n",
    "print(train['BN_binning'].value_counts())\n",
    "\n",
    "test_BN = test['BN'].values\n",
    "test_binning = pd.cut(test_BN, c, labels=range(range_num), include_lowest=True).astype(int)\n",
    "test['BN_binning'] = test_binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiply_features = [\n",
    "#     ['DI', 'DU'],\n",
    "    ['DU', 'DU'],\n",
    "    ['DU', 'FR'],\n",
    "    ['DA', 'DE'],\n",
    "    ['AB', 'GL'],\n",
    "]\n",
    "\n",
    "for j, columns_to_mul in enumerate(Multiply_features):\n",
    "    mix_col = columns_to_mul[0] + '+' + columns_to_mul[1]\n",
    "    train[mix_col] = train[columns_to_mul[0]] * train[columns_to_mul[1]]\n",
    "    test[mix_col] = test[columns_to_mul[0]] * test[columns_to_mul[1]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change(X):\n",
    "    X['out_GL'] = 0\n",
    "    X.loc[X['GL']<1,'out_GL'] = X.loc[X['GL']<1,'GL'].map(lambda x : x-X.loc[X['GL']<1,'GL'].mean())\n",
    "    X.loc[X['GL']>1.5,'out_GL'] = X.loc[X['GL']>1.5,'GL'].map(lambda x : x-X.loc[X['GL']>1.5,'GL'].mean())\n",
    "    X.out_GL = X.out_GL.astype('float')\n",
    "    X['DA*CS'] = np.log(X.DA*2 / X.CS**0.5)#0.2100892\\\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'AF', 'BQ', 'AB', 'DU', 'DI', 'FL', 'CR', 'DH', 'BN', 'DA', 'EH',\n",
      "       'CD ', 'BP', 'DL', 'EE', 'FD ', 'DE', 'GL', 'FR', 'FI', 'EB', 'CU',\n",
      "       'CS', 'BR', 'BZ', 'CC', 'Class', 'BN_binning', 'DU+DU', 'DU+FR',\n",
      "       'DA+DE', 'AB+GL', 'out_GL', 'DA*CS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train = change(train)\n",
    "test = change(test)\n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Poly_features = [\n",
    "                 ['DI', 'DU'],\n",
    "#                  ['BR', 'BZ'],\n",
    "#                  ['CR', 'AB', 'FL',],\n",
    "#                  ['CR'],\n",
    "                ]\n",
    "\n",
    "for j, columns_to_derive in enumerate(Poly_features):\n",
    "    # 多项式特征衍生\n",
    "    degree_dim = 3 if j == 0 else 2\n",
    "    poly = PolynomialFeatures(degree=degree_dim, include_bias=False, interaction_only=True)\n",
    "    # Z-Score标准化\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    poly_features = poly.fit_transform(train[columns_to_derive])\n",
    "    scaled_features = scaler.fit_transform(poly_features)\n",
    "    # 生成新的特征列名\n",
    "    new_feature_names = [f\"poly_{j}_{i}\" for i in range(scaled_features.shape[1])]\n",
    "    features_train_df = pd.DataFrame(scaled_features, columns=new_feature_names)\n",
    "    train = pd.concat([train, features_train_df], axis=1)\n",
    "    train = train.drop(columns=columns_to_derive)\n",
    "\n",
    "    # 测试集\n",
    "    poly_features_test = poly.transform(test[columns_to_derive])\n",
    "    scaled_features_test = scaler.transform(poly_features_test)\n",
    "    features_test_df = pd.DataFrame(scaled_features_test, columns=new_feature_names)\n",
    "    test = pd.concat([test, features_test_df], axis=1)\n",
    "    test = test.drop(columns=columns_to_derive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'AF', 'BQ', 'AB', 'FL', 'CR', 'DH', 'BN', 'DA', 'EH', 'CD ', 'BP', 'DL', 'EE', 'DE', 'GL', 'FR', 'FI', 'EB', 'CU', 'BR', 'BZ', 'CC', 'Class', 'BN_binning', 'DU+DU', 'DU+FR', 'DA+DE', 'AB+GL', 'out_GL', 'DA*CS', 'poly_0_0', 'poly_0_1', 'poly_0_2']\n",
      "(617, 34)\n"
     ]
    }
   ],
   "source": [
    "drop_col = [\n",
    "#     ['BC', 'CL'],\n",
    "    # ['EU', 'CW '],\n",
    "    # ['count_isnull'],\n",
    "    # ['feature_distance_0_0', 'feature_distance_0_1', 'feature_distance_0_2', 'feature_distance_0_3', 'feature_distance_0_4', 'feature_distance_0_5', 'feature_distance_0_6', 'feature_distance_0_7', 'feature_distance_0_8', 'feature_distance_0_9'],\n",
    "    # ['feature_distance_1_0', 'feature_distance_1_1', 'feature_distance_1_2', 'feature_distance_1_3', 'feature_distance_1_4', 'feature_distance_1_5', 'feature_distance_1_6', 'feature_distance_1_7', 'feature_distance_1_8', 'feature_distance_1_9'],\n",
    "    # ['BN_binning'],\n",
    "#     ['CF', 'AF', 'FE', 'CR', 'BR', 'GH', 'EE']\n",
    "    ['FD ','CS']\n",
    "]\n",
    "for dc in drop_col:\n",
    "    train = train.drop(columns=dc)\n",
    "    test = test.drop(columns=dc)\n",
    "    \n",
    "print(train.columns.tolist())\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_meta = greeks['Alpha']\n",
    "X = train.drop(columns=['Id', 'Class'])\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_meta, test_size=0.2, random_state=25)\n",
    "\n",
    "# 将标签'A', 'B', 'C', 'D'映射成0，1，2，3\n",
    "label_encoder = LabelEncoder()\n",
    "y_meta = label_encoder.fit_transform(y_meta)\n",
    "X = pd.DataFrame(X)\n",
    "y_meta = pd.DataFrame(y_meta)\n",
    "y = pd.DataFrame(y)\n",
    "# print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    # print(y_true)\n",
    "    nc = np.bincount(y_true)\n",
    "    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15, labels=[0, 1])\n",
    "\n",
    "def calc_loss(y_pred, y):\n",
    "    probabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)\n",
    "    p0 = probabilities[:, :1]       # 计算class=\n",
    "    p1 = 1 - p0\n",
    "\n",
    "    y = y.values\n",
    "    y = np.array([0 if x==0 else 1 for x in y])\n",
    "    loss = balanced_log_loss(y, p1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold as KF, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "\n",
    "\n",
    "cv_outer = SKF(n_splits = 8, shuffle=True, random_state=19)\n",
    "cv_inner = KF(n_splits = 5, shuffle=True, random_state=19)\n",
    "ros = RandomOverSampler(random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-06 21:56:59,486] A new study created in memory with name: no-name-654b2a94-4c07-41f7-8531-c21c193524b1\n",
      "[I 2023-08-06 21:57:04,735] Trial 0 finished with value: 0.2731414468026305 and parameters: {'lambda': 1.3982613466897604e-08, 'alpha': 0.023995060080478196, 'max_depth': 4, 'learning_rate': 0.07724597665443066, 'gamma': 8.81536891680552e-05, 'colsample_bytree': 0.4470793255494294, 'min_child_weight': 3, 'subsample': 0.7203651939640793}. Best is trial 0 with value: 0.2731414468026305.\n",
      "[I 2023-08-06 21:57:14,171] Trial 1 finished with value: 0.3935942673644262 and parameters: {'lambda': 0.00011637727484503234, 'alpha': 4.3763290397408136e-07, 'max_depth': 5, 'learning_rate': 0.012115963815083123, 'gamma': 0.04571692289554108, 'colsample_bytree': 0.7694808384852144, 'min_child_weight': 2, 'subsample': 0.43735587218744254}. Best is trial 0 with value: 0.2731414468026305.\n",
      "[I 2023-08-06 21:57:19,664] Trial 2 finished with value: 0.235522396207249 and parameters: {'lambda': 0.24368544958091196, 'alpha': 0.00031385249004930847, 'max_depth': 7, 'learning_rate': 0.07485679079850467, 'gamma': 4.054645233348994e-07, 'colsample_bytree': 0.9123804858046384, 'min_child_weight': 8, 'subsample': 0.3796439362711639}. Best is trial 2 with value: 0.235522396207249.\n",
      "[I 2023-08-06 21:57:23,704] Trial 3 finished with value: 0.5081016724752306 and parameters: {'lambda': 0.008293171946041532, 'alpha': 1.5448326237776533, 'max_depth': 5, 'learning_rate': 0.011103667514097384, 'gamma': 0.3132473621015831, 'colsample_bytree': 0.4987453269133868, 'min_child_weight': 8, 'subsample': 0.14702330723572538}. Best is trial 2 with value: 0.235522396207249.\n",
      "[I 2023-08-06 21:57:29,948] Trial 4 finished with value: 0.29825114134603 and parameters: {'lambda': 1.1453833120743218, 'alpha': 0.000200017011118345, 'max_depth': 5, 'learning_rate': 0.022194251059916077, 'gamma': 1.3146887941562644e-08, 'colsample_bytree': 0.6265050510483543, 'min_child_weight': 10, 'subsample': 0.6651669017162094}. Best is trial 2 with value: 0.235522396207249.\n",
      "[I 2023-08-06 21:57:36,435] Trial 5 finished with value: 0.30287403151878606 and parameters: {'lambda': 2.3787916856547222e-08, 'alpha': 3.6686072115675747, 'max_depth': 8, 'learning_rate': 0.022827787263856578, 'gamma': 0.1223070835681904, 'colsample_bytree': 0.894731309894195, 'min_child_weight': 8, 'subsample': 0.7423305583204326}. Best is trial 2 with value: 0.235522396207249.\n",
      "[I 2023-08-06 21:57:42,697] Trial 6 finished with value: 0.3237334306337563 and parameters: {'lambda': 0.031511724959663576, 'alpha': 9.219148428787545e-06, 'max_depth': 8, 'learning_rate': 0.07837579454169236, 'gamma': 0.00011482954833678461, 'colsample_bytree': 0.3592992706782502, 'min_child_weight': 1, 'subsample': 0.8609223681392406}. Best is trial 2 with value: 0.235522396207249.\n",
      "[I 2023-08-06 21:57:48,391] Trial 7 finished with value: 0.29982035953102837 and parameters: {'lambda': 0.042311256330377967, 'alpha': 1.4624792570001083e-08, 'max_depth': 3, 'learning_rate': 0.02420743985599691, 'gamma': 3.882214557104826e-07, 'colsample_bytree': 0.9898593077148118, 'min_child_weight': 10, 'subsample': 0.6135311761658542}. Best is trial 2 with value: 0.235522396207249.\n",
      "[I 2023-08-06 21:57:53,900] Trial 8 finished with value: 0.2521325445960145 and parameters: {'lambda': 1.6836263286036648e-08, 'alpha': 1.2503381640478828e-08, 'max_depth': 4, 'learning_rate': 0.03543690972329799, 'gamma': 1.4048803682881436e-07, 'colsample_bytree': 0.5030502374707725, 'min_child_weight': 10, 'subsample': 0.6176144105110614}. Best is trial 2 with value: 0.235522396207249.\n",
      "[I 2023-08-06 21:58:03,320] Trial 9 finished with value: 0.3470074684051557 and parameters: {'lambda': 1.144600489315241e-06, 'alpha': 3.086851885293534e-06, 'max_depth': 6, 'learning_rate': 0.015342900498094955, 'gamma': 0.011446764556643377, 'colsample_bytree': 0.8869828242712338, 'min_child_weight': 4, 'subsample': 0.42185688484524275}. Best is trial 2 with value: 0.235522396207249.\n",
      "[I 2023-08-06 21:58:08,445] Trial 10 finished with value: 0.2849038841839578 and parameters: {'lambda': 2.8369547227290046, 'alpha': 0.0022700083583442163, 'max_depth': 9, 'learning_rate': 0.050784265740899275, 'gamma': 4.7720044409285914e-06, 'colsample_bytree': 0.17010326400433717, 'min_child_weight': 6, 'subsample': 0.9731188513441866}. Best is trial 2 with value: 0.235522396207249.\n",
      "[I 2023-08-06 21:58:16,013] Trial 11 finished with value: 0.24373743666970665 and parameters: {'lambda': 9.209939317011544e-05, 'alpha': 1.3646089834552227e-08, 'max_depth': 7, 'learning_rate': 0.04350910465862693, 'gamma': 1.3404733198283052e-08, 'colsample_bytree': 0.6456852037579456, 'min_child_weight': 8, 'subsample': 0.4574706790241132}. Best is trial 2 with value: 0.235522396207249.\n",
      "[I 2023-08-06 21:58:22,851] Trial 12 finished with value: 0.23311389267465432 and parameters: {'lambda': 0.00014100428814842325, 'alpha': 7.833538136716478e-05, 'max_depth': 7, 'learning_rate': 0.053435220689925476, 'gamma': 1.2078817570510267e-08, 'colsample_bytree': 0.7002634283081766, 'min_child_weight': 7, 'subsample': 0.38596571042098154}. Best is trial 12 with value: 0.23311389267465432.\n",
      "[I 2023-08-06 21:58:27,690] Trial 13 finished with value: 0.23318682515825392 and parameters: {'lambda': 0.0010045401478539298, 'alpha': 0.00016362668481373385, 'max_depth': 7, 'learning_rate': 0.09981279352150048, 'gamma': 2.6384857053139054e-07, 'colsample_bytree': 0.74463558098416, 'min_child_weight': 6, 'subsample': 0.2862716373648142}. Best is trial 12 with value: 0.23311389267465432.\n",
      "[I 2023-08-06 21:58:32,276] Trial 14 finished with value: 0.22896226946632905 and parameters: {'lambda': 0.0009813637623286337, 'alpha': 4.392514309920445e-05, 'max_depth': 7, 'learning_rate': 0.09771417099049298, 'gamma': 1.013352335034362e-08, 'colsample_bytree': 0.7461441378047815, 'min_child_weight': 6, 'subsample': 0.21247446631541594}. Best is trial 14 with value: 0.22896226946632905.\n",
      "[I 2023-08-06 21:58:37,703] Trial 15 finished with value: 0.2360578706779362 and parameters: {'lambda': 1.0683740864727301e-05, 'alpha': 2.08168637320397e-05, 'max_depth': 9, 'learning_rate': 0.0552341672971675, 'gamma': 1.9410896215767446e-08, 'colsample_bytree': 0.7458343606351824, 'min_child_weight': 5, 'subsample': 0.12799677373019724}. Best is trial 14 with value: 0.22896226946632905.\n",
      "[I 2023-08-06 21:58:42,585] Trial 16 finished with value: 0.22893797754520231 and parameters: {'lambda': 0.0011463941023244048, 'alpha': 0.008490131614554145, 'max_depth': 6, 'learning_rate': 0.0981269884697499, 'gamma': 4.341866201304473e-06, 'colsample_bytree': 0.6274267217046212, 'min_child_weight': 7, 'subsample': 0.2490840568032594}. Best is trial 16 with value: 0.22893797754520231.\n",
      "[I 2023-08-06 21:58:47,689] Trial 17 finished with value: 0.22308590780030205 and parameters: {'lambda': 0.0027347399273500617, 'alpha': 0.005429919458425685, 'max_depth': 6, 'learning_rate': 0.09674549175065159, 'gamma': 1.084063579130809e-05, 'colsample_bytree': 0.590931446180438, 'min_child_weight': 5, 'subsample': 0.2475086459639063}. Best is trial 17 with value: 0.22308590780030205.\n",
      "[I 2023-08-06 21:58:53,672] Trial 18 finished with value: 0.2346131123031606 and parameters: {'lambda': 0.0026923670823657026, 'alpha': 0.012649609148478536, 'max_depth': 6, 'learning_rate': 0.06551727797334582, 'gamma': 1.1189194591549321e-05, 'colsample_bytree': 0.5816649902785234, 'min_child_weight': 4, 'subsample': 0.2638123324461321}. Best is trial 17 with value: 0.22308590780030205.\n",
      "[I 2023-08-06 21:58:57,115] Trial 19 finished with value: 0.22609082262877758 and parameters: {'lambda': 0.021961347432181956, 'alpha': 0.11472038211937598, 'max_depth': 6, 'learning_rate': 0.09859744755612464, 'gamma': 0.00021198992443427912, 'colsample_bytree': 0.4140107543374004, 'min_child_weight': 5, 'subsample': 0.10570908715937227}. Best is trial 17 with value: 0.22308590780030205.\n",
      "[I 2023-08-06 21:59:01,100] Trial 20 finished with value: 0.27481381906458907 and parameters: {'lambda': 0.13020988278525528, 'alpha': 0.28049216112432734, 'max_depth': 4, 'learning_rate': 0.04011552227572208, 'gamma': 0.0006438251223771346, 'colsample_bytree': 0.37066962783144863, 'min_child_weight': 4, 'subsample': 0.10304998017754083}. Best is trial 17 with value: 0.22308590780030205.\n",
      "[I 2023-08-06 21:59:05,987] Trial 21 finished with value: 0.22228525052031264 and parameters: {'lambda': 0.00901345080349895, 'alpha': 0.0033071952022651363, 'max_depth': 6, 'learning_rate': 0.09810251734210278, 'gamma': 1.0304707261873475e-05, 'colsample_bytree': 0.5680185801020932, 'min_child_weight': 5, 'subsample': 0.22095948389484144}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[I 2023-08-06 21:59:10,782] Trial 22 finished with value: 0.22728995198724672 and parameters: {'lambda': 0.01112567051721624, 'alpha': 0.11091511817721901, 'max_depth': 5, 'learning_rate': 0.06787179561494487, 'gamma': 0.001320274929214095, 'colsample_bytree': 0.5465866360138352, 'min_child_weight': 5, 'subsample': 0.18520550800982793}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[I 2023-08-06 21:59:15,358] Trial 23 finished with value: 0.2370068526869906 and parameters: {'lambda': 9.949294907522637, 'alpha': 0.004045318057841109, 'max_depth': 6, 'learning_rate': 0.08583577163418075, 'gamma': 2.786854409777983e-05, 'colsample_bytree': 0.4174786935231817, 'min_child_weight': 3, 'subsample': 0.31463091243166164}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[I 2023-08-06 21:59:18,155] Trial 24 finished with value: 0.24380346275820022 and parameters: {'lambda': 0.18510432225371942, 'alpha': 0.0012467962113339988, 'max_depth': 6, 'learning_rate': 0.0636138564537633, 'gamma': 0.00043549840857580815, 'colsample_bytree': 0.5633322016651747, 'min_child_weight': 5, 'subsample': 0.10012741339328446}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[I 2023-08-06 21:59:21,320] Trial 25 finished with value: 0.23680400258692416 and parameters: {'lambda': 0.004030107607526605, 'alpha': 0.0703321772562951, 'max_depth': 5, 'learning_rate': 0.08393138177221107, 'gamma': 4.094997110444844e-06, 'colsample_bytree': 0.2761788441294796, 'min_child_weight': 3, 'subsample': 0.2094897559964291}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[I 2023-08-06 21:59:25,871] Trial 26 finished with value: 0.24357662852102083 and parameters: {'lambda': 0.023197694777157277, 'alpha': 0.0009502381680879691, 'max_depth': 8, 'learning_rate': 0.06074218863753447, 'gamma': 3.591653292363861e-05, 'colsample_bytree': 0.48083643819606364, 'min_child_weight': 4, 'subsample': 0.3314360537953751}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[I 2023-08-06 21:59:28,724] Trial 27 finished with value: 0.22482292148883468 and parameters: {'lambda': 0.005107369225863768, 'alpha': 0.3340076331023124, 'max_depth': 6, 'learning_rate': 0.08509790145944411, 'gamma': 0.002634782680964027, 'colsample_bytree': 0.5381986224270003, 'min_child_weight': 7, 'subsample': 0.1705507408606542}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[I 2023-08-06 21:59:33,128] Trial 28 finished with value: 0.2418776117273611 and parameters: {'lambda': 0.003451482945413076, 'alpha': 0.6394462947934997, 'max_depth': 6, 'learning_rate': 0.06954554127052558, 'gamma': 0.002525030296590303, 'colsample_bytree': 0.5388415796375096, 'min_child_weight': 7, 'subsample': 0.5050145925957882}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[I 2023-08-06 21:59:35,805] Trial 29 finished with value: 0.22500083944047838 and parameters: {'lambda': 0.00034290031116413215, 'alpha': 0.024555242967541108, 'max_depth': 4, 'learning_rate': 0.0817385853143918, 'gamma': 8.080849668728996e-05, 'colsample_bytree': 0.4419037068932082, 'min_child_weight': 9, 'subsample': 0.18266449855597672}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[I 2023-08-06 21:59:39,067] Trial 30 finished with value: 0.23182353307785117 and parameters: {'lambda': 0.07646347191735885, 'alpha': 0.015764238504008213, 'max_depth': 3, 'learning_rate': 0.07638508114097393, 'gamma': 0.003913835581028374, 'colsample_bytree': 0.6719364700508813, 'min_child_weight': 6, 'subsample': 0.34143858272836153}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[I 2023-08-06 21:59:41,797] Trial 31 finished with value: 0.22716306904301167 and parameters: {'lambda': 0.0002478625539708251, 'alpha': 0.0391843737684179, 'max_depth': 4, 'learning_rate': 0.08442078700692981, 'gamma': 6.146482546169587e-05, 'colsample_bytree': 0.45832147982674754, 'min_child_weight': 9, 'subsample': 0.18465235535847294}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[I 2023-08-06 21:59:45,110] Trial 32 finished with value: 0.22545762223554924 and parameters: {'lambda': 0.00045498941875361473, 'alpha': 0.041536006256523365, 'max_depth': 5, 'learning_rate': 0.08568843496302263, 'gamma': 0.0001666406915686661, 'colsample_bytree': 0.5674467804498811, 'min_child_weight': 9, 'subsample': 0.23927559440725388}. Best is trial 21 with value: 0.22228525052031264.\n",
      "[W 2023-08-06 21:59:47,910] Trial 33 failed with parameters: {'lambda': 0.007601269208963785, 'alpha': 6.316977198099724, 'max_depth': 3, 'learning_rate': 0.07360743498492286, 'gamma': 2.0206606629610703e-05, 'colsample_bytree': 0.5184481079195653, 'min_child_weight': 9, 'subsample': 0.1746875090822362} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/kf/klhxzw8d44s1ggrx0yr5dr6w0000gn/T/ipykernel_47329/2366002266.py\", line 65, in objective\n",
      "    model.fit(out_X, out_y_meta, eval_set=[(x_val, y_val_meta)], early_stopping_rounds=100, verbose=False)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1490, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/training.py\", line 185, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py\", line 1918, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "KeyboardInterrupt\n",
      "[W 2023-08-06 21:59:47,913] Trial 33 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m# 创建 Optuna 优化器并运行优化\u001b[39;00m\n\u001b[1;32m     76\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n\u001b[1;32m     79\u001b[0m \u001b[39m# 打印调参结果\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m'\u001b[39m, study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/optuna/study/study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     _optimize(\n\u001b[1;32m    444\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    446\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    447\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    448\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    449\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    450\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    451\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    452\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    453\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[15], line 65\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     62\u001b[0m out_X, out_y_meta \u001b[39m=\u001b[39m x_train, y_train\n\u001b[1;32m     63\u001b[0m \u001b[39m# out_y = out_y_meta.apply(lambda x: 0 if x == 'A' else 1)\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m model\u001b[39m.\u001b[39;49mfit(out_X, out_y_meta, eval_set\u001b[39m=\u001b[39;49m[(x_val, y_val_meta)], early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m \u001b[39m# 用训练好的model计算x_val的loss\u001b[39;00m\n\u001b[1;32m     68\u001b[0m val_y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(x_val)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1462\u001b[0m (\n\u001b[1;32m   1463\u001b[0m     model,\n\u001b[1;32m   1464\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1470\u001b[0m )\n\u001b[1;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[1;32m   1488\u001b[0m )\n\u001b[0;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1491\u001b[0m     params,\n\u001b[1;32m   1492\u001b[0m     train_dmatrix,\n\u001b[1;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1502\u001b[0m )\n\u001b[1;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 定义目标函数\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        # 多分类XGBClassifier参数\n",
    "        # 'objective': 'multi:softprob',\n",
    "        'num_class': 4,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'verbosity': 0,\n",
    "        'booster': 'gbtree',\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.1, 1.0),\n",
    "    }\n",
    "\n",
    "    xgb_params = {\n",
    "            'learning_rate': 0.413327571405248,\n",
    "            'booster': 'gbtree',\n",
    "            'lambda': 0.0000263894617720096,\n",
    "            'alpha': 0.000463768723479341,\n",
    "            'subsample': 0.237467672874133,\n",
    "            'colsample_bytree': 0.618829300507829,\n",
    "            'max_depth': 5,\n",
    "            'min_child_weight': 9,\n",
    "            'eta': 2.09477807126539E-06,\n",
    "            'gamma': 0.000847289463422307,\n",
    "            'grow_policy': 'depthwise',\n",
    "            'n_jobs': -1,\n",
    "            'objective': 'multi:softprob',\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'verbosity': 0,\n",
    "        }\n",
    "\n",
    "    # 创建 XGBoost 分类器\n",
    "    model = XGBClassifier(**params)\n",
    "    train_preds = np.zeros((X.shape[0], 4))\n",
    "\n",
    "    for out_id, (train_idx, val_idx) in enumerate(cv_outer.split(X, y_meta), start=1):\n",
    "        x_train_ori, x_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_ori, y_val = y_meta.iloc[train_idx], y.iloc[val_idx]\n",
    "        y_val_meta = y_meta.iloc[val_idx]\n",
    "        y_train_label = y.iloc[train_idx]\n",
    "\n",
    "        cols_x_train_ori = len(x_train_ori.columns)\n",
    "        y_train_ori_df = pd.DataFrame(y_train_ori, columns=['Alpha'])\n",
    "        x_train_ori_comb = pd.concat((x_train_ori, y_train_ori_df), axis=1)\n",
    "\n",
    "#       train_ros, y_nonsense = ros.fit_resample(x_train_ori_comb, y_train_label)    # 按 0/1 over sample\n",
    "        x_train, y_train = ros.fit_resample(x_train_ori, y_train_ori)         # 按 A/B/D/G over sample\n",
    "        # x_train, y_train = x_train_ori, y_train_ori\n",
    "\n",
    "        out_X, out_y_meta = x_train, y_train\n",
    "        # out_y = out_y_meta.apply(lambda x: 0 if x == 'A' else 1)\n",
    "\n",
    "        model.fit(out_X, out_y_meta, eval_set=[(x_val, y_val_meta)], early_stopping_rounds=100, verbose=False)\n",
    "        \n",
    "        # 用训练好的model计算x_val的loss\n",
    "        val_y_pred = model.predict_proba(x_val)\n",
    "        train_preds[val_idx] = val_y_pred\n",
    "\n",
    "    bll = calc_loss(train_preds, y)\n",
    "\n",
    "    return bll\n",
    "\n",
    "# 创建 Optuna 优化器并运行优化\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=800)\n",
    "\n",
    "# 打印调参结果\n",
    "print('Best trial:', study.best_trial.params)\n",
    "print('Best Bll:', study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
