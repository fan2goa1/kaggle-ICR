{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-06-25T10:08:23.661832Z","iopub.status.busy":"2023-06-25T10:08:23.66068Z","iopub.status.idle":"2023-06-25T10:08:44.412009Z","shell.execute_reply":"2023-06-25T10:08:44.409631Z","shell.execute_reply.started":"2023-06-25T10:08:23.661787Z"}},"source":["ICR Competition"]},{"cell_type":"code","execution_count":1259,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T00:07:36.857723Z","iopub.status.busy":"2023-07-13T00:07:36.857261Z","iopub.status.idle":"2023-07-13T00:07:50.330860Z","shell.execute_reply":"2023-07-13T00:07:50.329524Z","shell.execute_reply.started":"2023-07-13T00:07:36.857690Z"},"trusted":true},"outputs":[],"source":["# !pip install tabpfn --no-index --find-links=file:///kaggle/input/pip-packages-icr/pip-packages\n","# !mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n","# !cp /kaggle/input/pip-packages-icr/pip-packages/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/"]},{"cell_type":"code","execution_count":1260,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-13T00:07:50.333692Z","iopub.status.busy":"2023-07-13T00:07:50.333302Z","iopub.status.idle":"2023-07-13T00:07:52.796769Z","shell.execute_reply":"2023-07-13T00:07:52.795554Z","shell.execute_reply.started":"2023-07-13T00:07:50.333659Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder,normalize\n","from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.impute import SimpleImputer\n","import imblearn\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.under_sampling import RandomUnderSampler\n","import xgboost\n","import inspect\n","from collections import defaultdict\n","from tabpfn import TabPFNClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["先加载各个数据文件"]},{"cell_type":"code","execution_count":1261,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T00:07:52.800548Z","iopub.status.busy":"2023-07-13T00:07:52.799459Z","iopub.status.idle":"2023-07-13T00:07:52.835269Z","shell.execute_reply":"2023-07-13T00:07:52.834379Z","shell.execute_reply.started":"2023-07-13T00:07:52.800501Z"},"trusted":true},"outputs":[],"source":["# train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n","# test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n","# sample = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n","# greeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\n","\n","train = pd.read_csv('./icr-identify-age-related-conditions/train.csv')\n","test = pd.read_csv('./icr-identify-age-related-conditions/test.csv')\n","sample = pd.read_csv('./icr-identify-age-related-conditions/sample_submission.csv')\n","greeks = pd.read_csv('./icr-identify-age-related-conditions/greeks.csv')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["然后将EJ属性二值化（0/1）。"]},{"cell_type":"code","execution_count":1262,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T00:07:52.837901Z","iopub.status.busy":"2023-07-13T00:07:52.837590Z","iopub.status.idle":"2023-07-13T00:07:52.846310Z","shell.execute_reply":"2023-07-13T00:07:52.845333Z","shell.execute_reply.started":"2023-07-13T00:07:52.837874Z"},"trusted":true},"outputs":[],"source":["first_category = train.EJ.unique()[0]\n","train.EJ = train.EJ.eq(first_category).astype('int')\n","test.EJ = test.EJ.eq(first_category).astype('int')\n","\n","origin_test = test.copy()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["根据Disscussion Post，我们可以将原数据集中的某些属性转化成整数取值，并且猜测BN属性可能是指年龄，所以我们可以在BN上进行一些操作。"]},{"cell_type":"code","execution_count":1263,"metadata":{},"outputs":[],"source":["int_denominators = {\n","    'AB': 0.004273,\n","    'AF': 0.00242,\n","    'AH': 0.008709,\n","    'AM': 0.003097,\n","    'AR': 0.005244,\n","    'AX': 0.008859,\n","    'AY': 0.000609,\n","    'AZ': 0.006302,\n","    'BC': 0.007028,\n","    'BD ': 0.00799,\n","    'BN': 0.3531,\n","    'BP': 0.004239,\n","    'BQ': 0.002605,\n","    'BR': 0.006049,\n","    'BZ': 0.004267,\n","    'CB': 0.009191,\n","    'CC': 6.12e-06,\n","    'CD ': 0.007928,\n","    'CF': 0.003041,\n","    'CH': 0.000398,\n","    'CL': 0.006365,\n","    'CR': 7.5e-05,\n","    'CS': 0.003487,\n","    'CU': 0.005517,\n","    'CW ': 9.2e-05,\n","    'DA': 0.00388,\n","    'DE': 0.004435,\n","    'DF': 0.000351,\n","    'DH': 0.002733,\n","    'DI': 0.003765,\n","    'DL': 0.00212,\n","    'DN': 0.003412,\n","    'DU': 0.0013794,\n","    'DV': 0.00259,\n","    'DY': 0.004492,\n","    'EB': 0.007068,\n","    'EE': 0.004031,\n","    'EG': 0.006025,\n","    'EH': 0.006084,\n","    'EL': 0.000429,\n","    'EP': 0.009269,\n","    'EU': 0.005064,\n","    'FC': 0.005712,\n","    'FD ': 0.005937,\n","    'FE': 0.007486,\n","    'FI': 0.005513,\n","    'FR': 0.00058,\n","    'FS': 0.006773,\n","    'GB': 0.009302,\n","    'GE': 0.004417,\n","    'GF': 0.004374,\n","    'GH': 0.003721,\n","    'GI': 0.002572\n","}\n","for k, v in int_denominators.items():\n","    train[k] = np.round(train[k] / v, 1)\n","    test[k] = np.round(test[k] / v, 1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["通过绘制密度-年龄图我们可以看到，Class为1的样本对应的曲线比Class为0的样本对应的曲线要右移了一些，所以这体现出年龄较大时更可能得病。为了增强BN这一属性对预测结果的影响，我们尝试将BN属性复制多份加入到数据中。\n","\n","predictor_columns中提取中56个属性特征。"]},{"cell_type":"code","execution_count":1264,"metadata":{},"outputs":[],"source":["repeat_attr = ['BN', 'CR', 'CU', 'DE']\n","for attr in repeat_attr:\n","    for i in range(3):\n","        AttrName = attr + str(i+1)\n","        train[AttrName] = train[attr].copy()\n","        test[AttrName] = test[attr].copy()\n","\n","predictor_columns = [n for n in train.columns if n != 'Class' and n != 'Id']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["设置评判标准。在这次比赛中使用的评判标准是balanced log loss，公式如下：\n","$$\n","\\text { Log Loss }=\\frac{-\\frac{1}{N_0} \\sum_{i=1}^{N_0} y_{0 i} \\log p_{0 i}-\\frac{1}{N_1} \\sum_{i=1}^{N_1} y_{1 i} \\log p_{1 i}}{2}\n","$$\n","这样的目标是平衡两类的重要程度。"]},{"cell_type":"code","execution_count":1265,"metadata":{},"outputs":[],"source":["from sklearn.metrics import log_loss\n","def balanced_log_loss(y_true, y_pred):\n","    nc = np.bincount(y_true)\n","    # print(1/nc[y_true])\n","    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15, labels=[0, 1])"]},{"cell_type":"code","execution_count":1266,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["17.26978799617044\n"]}],"source":["y_true = np.array([1,1,1,0,0,0,0]).astype('int')\n","y_pred = np.array([1] * len(y_true)).astype('float64')\n","# y_pred = np.array([1,1,1,0,0,0,0]).astype('float64')\n","bll = balanced_log_loss(y_true, y_pred)\n","print(bll)"]},{"cell_type":"code","execution_count":1267,"metadata":{},"outputs":[],"source":["from datetime import datetime\n","times = greeks.Epsilon.copy()\n","times[greeks.Epsilon != 'Unknown'] = greeks.Epsilon[greeks.Epsilon != 'Unknown'].map(lambda x: datetime.strptime(x,'%m/%d/%Y').toordinal())\n","times[greeks.Epsilon == 'Unknown'] = np.nan\n","# times = times.astype('float64')"]},{"cell_type":"code","execution_count":1268,"metadata":{},"outputs":[],"source":["train_pred_and_time = pd.concat((train, times, greeks.Alpha), axis=1)\n","train_label = train_pred_and_time['Class']\n","train_cate = train_pred_and_time.iloc[:, -1]         # A, B, D, G\n","train_pred_and_time = train_pred_and_time.drop(train_pred_and_time.columns[-1], axis=1)\n","\n","test_predictors = test[predictor_columns]\n","test_time = np.zeros((len(test_predictors), 1)) + train_pred_and_time.Epsilon.max() + 1\n","test_pred_and_time = pd.concat((test_predictors, pd.DataFrame(test_time, columns=['Epsilon'])), axis=1)"]},{"cell_type":"code","execution_count":1269,"metadata":{},"outputs":[],"source":["# 对train和test dataset的新的改动\n","\n","train_pred_and_time = train_pred_and_time.drop(['EH'], axis=1)\n","test_pred_and_time = test_pred_and_time.drop(['EH'], axis=1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["设计集成模型，这里使用了4个分类器，两个XGBoost，两个TabPFN。"]},{"cell_type":"code","execution_count":1270,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T00:07:52.881319Z","iopub.status.busy":"2023-07-13T00:07:52.880925Z","iopub.status.idle":"2023-07-13T00:07:52.894266Z","shell.execute_reply":"2023-07-13T00:07:52.893153Z","shell.execute_reply.started":"2023-07-13T00:07:52.881286Z"},"trusted":true},"outputs":[],"source":["class Ensemble():\n","    def __init__(self):\n","        self.imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n","\n","        self.classifiers =[xgboost.XGBClassifier(n_estimators=100,max_depth=3,learning_rate=0.15,subsample=0.9,colsample_bytree=0.85),\n","                           xgboost.XGBClassifier(),\n","                           TabPFNClassifier(N_ensemble_configurations=24),\n","                           TabPFNClassifier(N_ensemble_configurations=64)]\n","    \n","    def fit(self, X, y):\n","        y = y.values\n","        unique_classes, y = np.unique(y, return_inverse=True)\n","        self.classes_ = unique_classes\n","        # first_category = X.EJ.unique()[0]\n","        # X.EJ = X.EJ.eq(first_category).astype('int')\n","        \n","        X = self.imputer.fit_transform(X)\n","\n","        for classifier in self.classifiers:\n","            if classifier == self.classifiers[2] or classifier == self.classifiers[3]:\n","                classifier.fit(X, y, overwrite_warning=True)\n","            else :\n","                classifier.fit(X, y)\n","     \n","    def predict_proba(self, x):\n","        x = self.imputer.transform(x)\n","\n","        probabilities = np.stack([classifier.predict_proba(x) for classifier in self.classifiers])\n","        averaged_probabilities = np.mean(probabilities, axis=0)\n","        class_0_est_instances = averaged_probabilities[:, 0].sum()\n","        others_est_instances = averaged_probabilities[:, 1:].sum()\n","        \n","        # Weighted probabilities based on class imbalance\n","        new_probabilities = averaged_probabilities * np.array([[1/(class_0_est_instances if i==0 else others_est_instances) for i in range(averaged_probabilities.shape[1])]])\n","        \n","        return new_probabilities / np.sum(new_probabilities, axis=1, keepdims=1) "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["这里我们设置了两个KFold，一个为outer，用于选80%的trainning dataset和20%的validation dataset；\n","一个为inner，用于对trainning dataset分成5折，分别训练出5个模型（5-折模型）。用5-折模型对outer分出的20%的validation dataset预测并计算balanced log loss。\n","最后选取效果最好的5-折模型对test预测（即分别用5个模型预测，取均值）"]},{"cell_type":"code","execution_count":1271,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold as SKF, GridSearchCV\n","\n","cv_outer = SKF(n_splits = 5, shuffle=True, random_state=42)\n","# cv_inner = SKF(n_splits = 5, shuffle=True, random_state=19)"]},{"cell_type":"code","execution_count":1272,"metadata":{},"outputs":[],"source":["def calc_acc(y_pred, y):\n","    probabilities = np.concatenate((y_pred[:, :1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)\n","    p0 = probabilities[:, :1]       # 计算class=0的概率\n","    p1 = 1 - p0\n","    \n","    y = y.values.astype(int)\n","    cnt = 0\n","\n","    for i in range(len(p0)):\n","        if p0[i] >= p1[i]:\n","            lab = 0\n","        else :\n","            lab = 1\n","\n","        if lab == y[i]:\n","            cnt += 1\n","\n","    return cnt / len(p0)"]},{"cell_type":"code","execution_count":1273,"metadata":{},"outputs":[],"source":["def calc_loss(y_pred, y):\n","    probabilities = np.concatenate((y_pred[:, :1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)\n","    p0 = probabilities[:, :1]       # 计算class=0的概率\n","    p0[p0 > 0.75] = 1\n","    p0[p0 < 0.20] = 0\n","\n","    p1 = 1 - p0\n","\n","    y_  = y.values.astype(int).reshape(-1, 1)\n","    prt = np.concatenate((p0, p1, y_), axis=1)\n","    # print(prt)\n","    \n","    y = y.values.astype(int)\n","    loss = balanced_log_loss(y, p1)\n","\n","    return loss"]},{"cell_type":"code","execution_count":1274,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T00:07:52.896271Z","iopub.status.busy":"2023-07-13T00:07:52.895867Z","iopub.status.idle":"2023-07-13T00:07:52.979444Z","shell.execute_reply":"2023-07-13T00:07:52.978170Z","shell.execute_reply.started":"2023-07-13T00:07:52.896232Z"},"trusted":true},"outputs":[],"source":["from tqdm.notebook import tqdm\n","\n","def training(model, x, y, y_meta):\n","\n","    model.fit(x, y_meta)\n","    y_pred = model.predict_proba(x)\n","    metric = calc_loss(y_pred, y)\n","    acc = calc_acc(y_pred, y)\n","\n","    return model, metric, acc\n","    "]},{"cell_type":"code","execution_count":1275,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T00:07:53.093640Z","iopub.status.busy":"2023-07-13T00:07:53.092906Z","iopub.status.idle":"2023-07-13T00:07:54.195502Z","shell.execute_reply":"2023-07-13T00:07:54.194342Z","shell.execute_reply.started":"2023-07-13T00:07:53.093596Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading model that can be used for inference only\n","Using a Transformer with 25.82 M parameters\n","Loading model that can be used for inference only\n","Using a Transformer with 25.82 M parameters\n","第1折验证：\n","训练集loss: 9.992007221626415e-16; 训练集acc: 1.0\n","验证集loss: 0.2525410147871475; 验证集acc: 0.9516129032258065\n","\n","Saved new best model.\n","第2折验证：\n","训练集loss: 9.992007221626415e-16; 训练集acc: 1.0\n","验证集loss: 1.0762323246738559; 验证集acc: 0.9193548387096774\n","\n","第3折验证：\n","训练集loss: 9.992007221626413e-16; 训练集acc: 1.0\n","验证集loss: 1.37848762318333; 验证集acc: 0.9186991869918699\n","\n","第4折验证：\n","训练集loss: 9.992007221626415e-16; 训练集acc: 1.0\n","验证集loss: 3.4042875188636064; 验证集acc: 0.8617886178861789\n","\n","第5折验证：\n","训练集loss: 9.992007221626415e-16; 训练集acc: 1.0\n","验证集loss: 0.6252899985967869; 验证集acc: 0.9105691056910569\n","\n"]}],"source":["yt = Ensemble()\n","ros = RandomOverSampler(random_state=42)\n","\n","low_loss = np.inf\n","low_loss = np.inf\n","for out_id, (train_idx, val_idx) in enumerate(cv_outer.split(train_pred_and_time, train_label), start=1):\n","    # 初步得到训练集\n","    x_train = train_pred_and_time.iloc[train_idx].drop(['Class', 'Id'], axis=1)\n","    y_meta_train = train_cate[train_idx]\n","    y_train = train_label[train_idx]\n","\n","    # 为了平衡两类的样本数，进行过采样，得到最终训练集\n","    x_train_ros, y_meta_train_ros = ros.fit_resample(x_train, y_meta_train)\n","    y_train_ros = y_meta_train_ros.apply(lambda x: 0 if x == 'A' else 1)\n","\n","    # print(f'Original dataset shape:')\n","    # print(y_meta_train.value_counts())\n","    # print('Resample dataset shape')\n","    # print(y_meta_train_ros.value_counts())\n","\n","    # 得到验证集\n","    x_val = train_pred_and_time.iloc[val_idx].drop(['Class', 'Id'], axis=1)\n","    y_val = train_label.iloc[val_idx]\n","    y_meta_val = train_cate[val_idx]\n","\n","    # 模型训练\n","    model, train_loss, train_acc = training(yt, x_train_ros, y_train_ros, y_meta_train_ros)\n","    # model, train_loss, train_acc = training(yt, x_train, y_train, y_meta_train)\n","    \n","    # 模型验证\n","    y_pred = model.predict_proba(x_val)\n","    val_loss = calc_loss(y_pred, y_val)\n","    val_acc = calc_acc(y_pred, y_val)\n","\n","    # 输出结果\n","    print(f'第{out_id}折验证：')\n","    print(f'训练集loss: {train_loss}; 训练集acc: {train_acc}')\n","    print(f'验证集loss: {val_loss}; 验证集acc: {val_acc}\\n')\n","\n","    # 更新模型\n","    if val_loss < low_loss:\n","        low_loss = val_loss\n","        best_model = model\n","        print('Saved new best model.')\n","    \n","    # break\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["将训练得到的好模型保存下来，存入pkl文件"]},{"cell_type":"code","execution_count":1276,"metadata":{},"outputs":[],"source":["import pickle\n","\n","filename = 'model.pkl'\n","with open(filename, 'wb') as f:\n","    pickle.dump(model, f)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["用模型对验证集在本地进行指标评估(balanced log loss)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["对测试集进行预测，并输出到submission.csv"]},{"cell_type":"code","execution_count":1277,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T00:54:32.120471Z","iopub.status.busy":"2023-07-13T00:54:32.120030Z","iopub.status.idle":"2023-07-13T00:56:50.733258Z","shell.execute_reply":"2023-07-13T00:56:50.732272Z","shell.execute_reply.started":"2023-07-13T00:54:32.120434Z"},"trusted":true},"outputs":[],"source":["y_pred = model.predict_proba(test_pred_and_time)\n","\n","probabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:,1:], 1, keepdims=True)), axis=1)\n","p0 = probabilities[:,:1]\n","p0[p0 > 0.75] = 1\n","p0[p0 < 0.20] = 0\n","\n","# 根据discussion中的发现，BQ为none的class为0\n","p0[test_pred_and_time['BQ'] == None] = 0"]},{"cell_type":"code","execution_count":1278,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T00:57:11.934120Z","iopub.status.busy":"2023-07-13T00:57:11.933090Z","iopub.status.idle":"2023-07-13T00:57:11.945506Z","shell.execute_reply":"2023-07-13T00:57:11.944010Z","shell.execute_reply.started":"2023-07-13T00:57:11.934055Z"},"trusted":true},"outputs":[],"source":["submission = pd.DataFrame(test[\"Id\"], columns=[\"Id\"])\n","submission[\"class_0\"] = p0\n","submission[\"class_1\"] = 1 - p0\n","submission.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":1279,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T00:57:14.459872Z","iopub.status.busy":"2023-07-13T00:57:14.459469Z","iopub.status.idle":"2023-07-13T00:57:14.484732Z","shell.execute_reply":"2023-07-13T00:57:14.483411Z","shell.execute_reply.started":"2023-07-13T00:57:14.459839Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>class_0</th>\n","      <th>class_1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00eed32682bb</td>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>010ebe33f668</td>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>02fa521e1838</td>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>040e15f562a2</td>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>046e85c7cc7f</td>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             Id  class_0  class_1\n","0  00eed32682bb      0.5      0.5\n","1  010ebe33f668      0.5      0.5\n","2  02fa521e1838      0.5      0.5\n","3  040e15f562a2      0.5      0.5\n","4  046e85c7cc7f      0.5      0.5"]},"execution_count":1279,"metadata":{},"output_type":"execute_result"}],"source":["submission_df = pd.read_csv('submission.csv')\n","submission_df"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":4}
